<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Hybrid Coaching - <%= roomId %></title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: #1a1a1a;
            color: #fff;
            height: 100vh;
            overflow: hidden;
            /* Fix for iOS scrolling */
            -webkit-overflow-scrolling: touch;
        }

        .main-container {
            display: flex;
            height: 100vh;
        }

        /* Left panel - Video area */
        .video-panel {
            flex: 2;
            display: flex;
            flex-direction: column;
            background: #000;
        }

        .video-header {
            background: #2a2a2a;
            padding: 1rem;
            display: flex;
            justify-content: space-between;
            align-items: center;
            border-bottom: 1px solid #404040;
        }

        .room-info h2 {
            font-size: 1.2rem;
            margin-bottom: 0.25rem;
        }

        .room-info .subtitle {
            color: #aaa;
            font-size: 0.9rem;
        }

        .ai-status {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .ai-indicator {
            width: 12px;
            height: 12px;
            border-radius: 50%;
            background: #28a745;
            animation: pulse 2s infinite;
        }

        .ai-indicator.speaking {
            background: #17a2b8;
            animation: pulse 0.5s infinite;
        }

        .ai-indicator.paused {
            background: #ffc107;
            animation: none;
        }

        @keyframes pulse {
            0% { opacity: 1; }
            50% { opacity: 0.5; }
            100% { opacity: 1; }
        }

        .video-grid {
            flex: 1;
            display: flex;
            justify-content: center;
            align-items: center;
            gap: 2rem;
            padding: 2rem;
            background: #111;
            flex-wrap: wrap;
            overflow-y: auto;
            -webkit-overflow-scrolling: touch;
        }

        .video-wrapper {
            position: relative;
            background: #000;
            border-radius: 12px;
            overflow: hidden;
            border: 2px solid #333;
            /* Responsive sizing with max constraints */
            width: 100%;
            max-width: 300px;
            height: auto;
            aspect-ratio: 4/3;
        }

        .video-wrapper.speaking {
            border-color: #17a2b8;
            box-shadow: 0 0 20px rgba(23, 162, 184, 0.5);
        }

        /* Portrait mode (mobile/vertical) */
        @media (orientation: portrait) {
            .video-wrapper {
                max-width: 250px;
                max-height: 400px;
                aspect-ratio: 3/4;
            }
        }

        video {
            width: 100%;
            height: 100%;
            object-fit: cover;
        }

        .video-label {
            position: absolute;
            bottom: 10px;
            left: 10px;
            background: rgba(0,0,0,0.8);
            padding: 5px 10px;
            border-radius: 4px;
            font-size: 0.85rem;
        }

        .ai-avatar {
            display: flex;
            align-items: center;
            justify-content: center;
            background: linear-gradient(45deg, #667eea 0%, #764ba2 100%);
            position: relative;
            width: 100%;
            height: 100%;
        }

        .ai-sphere {
            width: 120px;
            height: 120px;
            border-radius: 50%;
            background: radial-gradient(circle at 30% 30%, #ffffff, #667eea, #764ba2);
            animation: rotate 20s linear infinite;
            position: relative;
            transition: all 0.3s ease;
        }

        .ai-sphere.speaking {
            animation: rotate 2s linear infinite, glow 0.8s ease-in-out infinite alternate, pulse-size 1.2s ease-in-out infinite alternate;
            background: radial-gradient(circle at 30% 30%, #ffffff, #17a2b8, #667eea, #764ba2);
        }

        .ai-sphere.listening {
            animation: rotate 20s linear infinite, subtle-glow 3s ease-in-out infinite alternate;
        }

        .ai-sphere.thinking {
            animation: rotate 5s linear infinite, thinking-pulse 1.5s ease-in-out infinite alternate;
            background: radial-gradient(circle at 30% 30%, #ffffff, #ffc107, #667eea, #764ba2);
        }

        @keyframes rotate {
            from { transform: rotate(0deg); }
            to { transform: rotate(360deg); }
        }

        @keyframes glow {
            from { 
                box-shadow: 0 0 20px rgba(23, 162, 184, 0.5);
                filter: brightness(1);
            }
            to { 
                box-shadow: 0 0 40px rgba(23, 162, 184, 1), 0 0 60px rgba(23, 162, 184, 0.5);
                filter: brightness(1.3);
            }
        }

        @keyframes pulse-size {
            from { 
                transform: scale(1) rotate(0deg);
            }
            to { 
                transform: scale(1.1) rotate(180deg);
            }
        }

        @keyframes subtle-glow {
            from { 
                box-shadow: 0 0 10px rgba(102, 126, 234, 0.3);
            }
            to { 
                box-shadow: 0 0 20px rgba(102, 126, 234, 0.6);
            }
        }

        @keyframes thinking-pulse {
            from { 
                box-shadow: 0 0 15px rgba(255, 193, 7, 0.4);
                filter: brightness(1);
            }
            to { 
                box-shadow: 0 0 30px rgba(255, 193, 7, 0.8);
                filter: brightness(1.2);
            }
        }

        /* Audio-reactive visualization elements */
        .ai-sphere::before {
            content: '';
            position: absolute;
            top: -10px;
            left: -10px;
            right: -10px;
            bottom: -10px;
            border-radius: 50%;
            background: radial-gradient(circle, transparent 60%, rgba(23, 162, 184, 0.1) 70%, transparent 80%);
            opacity: 0;
            transition: opacity 0.3s ease;
        }

        .ai-sphere.speaking::before {
            opacity: 1;
            animation: audio-ripple 1s ease-out infinite;
        }

        @keyframes audio-ripple {
            0% {
                transform: scale(0.8);
                opacity: 1;
            }
            100% {
                transform: scale(1.5);
                opacity: 0;
            }
        }

        .controls {
            background: #2a2a2a;
            padding: 1rem;
            display: flex;
            justify-content: center;
            gap: 1rem;
            border-top: 1px solid #404040;
        }

        .btn {
            background: #4a4a4a;
            border: none;
            color: #fff;
            padding: 10px 20px;
            border-radius: 6px;
            cursor: pointer;
            font-size: 14px;
            transition: all 0.2s;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .btn:hover {
            background: #5a5a5a;
            transform: translateY(-1px);
        }

        .btn.active {
            background: #dc3545;
        }

        .btn.ai-control {
            background: #17a2b8;
        }

        .btn.ai-control:hover {
            background: #138496;
        }

        .btn.ai-control.paused {
            background: #ffc107;
            color: #000;
        }

        /* Right panel - AI Controls & Transcript (Coach only) */
        .ai-panel {
            flex: 1;
            background: #1e1e1e;
            border-left: 1px solid #404040;
            display: flex;
            flex-direction: column;
            max-width: 400px;
        }

        .ai-panel.client-view {
            display: none;
        }

        .panel-header {
            background: #2a2a2a;
            padding: 1rem;
            border-bottom: 1px solid #404040;
        }

        .panel-header h3 {
            font-size: 1.1rem;
            margin-bottom: 0.5rem;
        }

        .ai-controls {
            display: flex;
            gap: 0.5rem;
            margin-bottom: 1rem;
        }

        .transcript-container {
            flex: 1;
            display: flex;
            flex-direction: column;
            overflow: hidden;
        }

        .transcript-tabs {
            display: flex;
            background: #333;
        }

        .tab {
            flex: 1;
            padding: 0.75rem;
            background: #333;
            border: none;
            color: #ccc;
            cursor: pointer;
            font-size: 0.9rem;
        }

        .tab.active {
            background: #1e1e1e;
            color: #fff;
            border-bottom: 2px solid #17a2b8;
        }

        .transcript-content {
            flex: 1;
            overflow-y: auto;
            padding: 1rem;
        }

        .transcript-item {
            margin-bottom: 1rem;
            padding: 0.75rem;
            border-radius: 8px;
            font-size: 0.9rem;
            line-height: 1.4;
        }

        .transcript-item.client {
            background: #2a4a2a;
            border-left: 4px solid #28a745;
        }

        .transcript-item.ai {
            background: #2a3a4a;
            border-left: 4px solid #17a2b8;
        }

        .transcript-item.coach {
            background: #4a2a2a;
            border-left: 4px solid #dc3545;
        }

        .transcript-item .speaker {
            font-weight: bold;
            margin-bottom: 0.25rem;
            font-size: 0.8rem;
            opacity: 0.8;
        }

        .transcript-item .timestamp {
            font-size: 0.75rem;
            opacity: 0.6;
            float: right;
        }

        .ai-response-preview {
            background: #333;
            padding: 1rem;
            border-top: 1px solid #404040;
            min-height: 100px;
        }

        .ai-response-preview h4 {
            font-size: 0.9rem;
            margin-bottom: 0.5rem;
            color: #17a2b8;
        }

        .ai-response-text {
            font-size: 0.85rem;
            line-height: 1.4;
            color: #ccc;
            font-style: italic;
        }

        .status-indicator {
            position: fixed;
            top: 20px;
            right: 20px;
            background: rgba(0,0,0,0.8);
            padding: 10px 20px;
            border-radius: 6px;
            font-size: 0.9rem;
            z-index: 1000;
        }

        .status-indicator.error {
            background: rgba(220, 53, 69, 0.9);
        }

        .status-indicator.success {
            background: rgba(40, 167, 69, 0.9);
        }

        /* Voice detection indicator */
        .voice-indicator {
            position: fixed;
            bottom: 20px;
            right: 20px;
            background: rgba(0,0,0,0.8);
            padding: 10px 15px;
            border-radius: 6px;
            font-size: 0.8rem;
            z-index: 1000;
            display: none;
            border-left: 3px solid #666;
        }

        .voice-indicator.detecting {
            border-left-color: #ffc107;
            background: rgba(255, 193, 7, 0.2);
        }

        .voice-indicator.speaking {
            border-left-color: #28a745;
            background: rgba(40, 167, 69, 0.2);
        }

        .voice-indicator.noise {
            border-left-color: #dc3545;
            background: rgba(220, 53, 69, 0.2);
        }

        /* Mobile responsiveness */
        @media (max-width: 1024px) {
            .main-container {
                flex-direction: column;
            }
            
            .ai-panel {
                max-width: none;
                height: 300px;
            }
            
            .video-grid {
                gap: 1rem;
                padding: 1rem;
            }
            
            .video-wrapper {
                max-width: 280px;
            }
        }

        @media (max-width: 768px) {
            body {
                overflow: auto;
                height: auto;
            }
            
            .main-container {
                height: auto;
                min-height: 100vh;
                overflow: visible;
            }
            
            .video-panel {
                height: auto;
                overflow: visible;
            }
            
            .ai-panel {
                height: 250px;
                position: sticky;
                top: 0;
                z-index: 10;
            }
            
            .video-grid {
                gap: 0.75rem;
                padding: 0.75rem;
                overflow: visible;
                height: auto;
            }
            
            .video-wrapper {
                max-width: 250px;
            }
            
            .ai-sphere {
                width: 100px;
                height: 100px;
            }
            
            .controls {
                flex-wrap: wrap;
                gap: 0.5rem;
                padding: 0.75rem;
            }
            
            .btn {
                flex: 1;
                min-width: 100px;
                padding: 8px 16px;
                font-size: 13px;
            }
            
            .video-header {
                padding: 0.75rem;
            }
            
            .room-info h2 {
                font-size: 1rem;
            }
            
            .room-info .subtitle {
                font-size: 0.8rem;
            }
        }

        @media (max-width: 480px) {
            .video-wrapper {
                max-width: 220px;
            }
            
            .ai-sphere {
                width: 80px;
                height: 80px;
            }
            
            .btn {
                min-width: 80px;
                padding: 6px 12px;
                font-size: 12px;
            }
            
            .video-grid {
                gap: 0.5rem;
                padding: 0.5rem;
            }
        }
    </style>
</head>
<body>
    <div class="main-container">
        <!-- Video Panel -->
        <div class="video-panel">
            <div class="video-header">
                <div class="room-info">
                    <h2>AI Hybrid Coaching</h2>
                    <div class="subtitle">Room: <%= roomId.substring(0, 8) %>... | Session: <%= sessionId %></div>
                </div>
                <div class="ai-status">
                    <div class="ai-indicator" id="aiIndicator"></div>
                    <span id="aiStatusText">AI Ready</span>
                </div>
            </div>

            <div class="video-grid">
                <div class="video-wrapper" id="localVideoWrapper">
                    <video id="localVideo" autoplay playsinline muted></video>
                    <div class="video-label">You (<%= user.displayName || user.email %>)</div>
                </div>

                <div class="video-wrapper" id="remoteVideoWrapper">
                    <video id="remoteVideo" autoplay playsinline></video>
                    <div class="video-label" id="remoteLabel">
                        <% if (user.role === 'coach') { %>
                            Waiting for client...
                        <% } else { %>
                            Waiting for coach...
                        <% } %>
                    </div>
                </div>

                <div class="video-wrapper" id="aiVideoWrapper">
                    <div class="ai-avatar">
                        <div class="ai-sphere" id="aiSphere"></div>
                    </div>
                    <div class="video-label">AI Coach Assistant</div>
                </div>
            </div>

            <div class="controls">
                <button id="toggleVideo" class="btn">üìπ Video</button>
                <button id="toggleAudio" class="btn">üé§ Audio</button>
                <button id="shareScreen" class="btn">üñ•Ô∏è Share Screen</button>
                
                <% if (user.role === 'coach') { %>
                    <button id="pauseAI" class="btn ai-control">‚è∏Ô∏è Pause AI</button>
                    <button id="resumeAI" class="btn ai-control" style="display: none;">‚ñ∂Ô∏è Resume AI</button>
                <% } %>
                
                <a href="/dashboard" class="btn" style="text-decoration: none;">üö™ Exit</a>
            </div>
        </div>

        <!-- AI Panel (Coach View Only) -->
        <% if (user.role === 'coach') { %>
        <div class="ai-panel">
            <div class="panel-header">
                <h3>AI Control Center</h3>
                <div class="ai-controls">
                    <button id="quickPause" class="btn ai-control" style="font-size: 0.8rem;">Quick Pause</button>
                    <button id="testAudio" class="btn" style="font-size: 0.8rem;">üé§ Test</button>
                    <button id="aiSettings" class="btn" style="font-size: 0.8rem;">‚öôÔ∏è Settings</button>
                </div>
            </div>

            <div class="transcript-container">
                <div class="transcript-tabs">
                    <button class="tab active" data-tab="live">Live Transcript</button>
                    <button class="tab" data-tab="ai">AI Responses</button>
                </div>

                <div class="transcript-content" id="transcriptContent">
                    <div class="transcript-item ai">
                        <div class="speaker">AI Assistant <span class="timestamp">Session Start</span></div>
                        <div>Hello! I'm ready to assist with this coaching session. I'll help facilitate the conversation and provide insights as needed.</div>
                    </div>
                </div>
            </div>

            <div class="ai-response-preview">
                <h4>Next AI Response Preview:</h4>
                <div class="ai-response-text" id="aiResponsePreview">
                    Waiting for client input...
                </div>
            </div>
        </div>
        <% } %>
    </div>

    <div id="statusIndicator" class="status-indicator" style="display: none;"></div>
    <div id="voiceIndicator" class="voice-indicator">
        <div>üé§ Audio Level: <span id="audioLevel">0</span></div>
        <div>üîä Voice: <span id="voiceStatus">Listening...</span></div>
    </div>

    <script>
        // Configuration
        const roomId = "<%= roomId %>";
        const sessionId = "<%= sessionId %>";
        const userRole = "<%= user.role %>";
        const userName = "<%= user.displayName || user.email %>";
        const userId = "<%= user.id %>";
        
        // State management
        let localStream = null;
        let peerConnection = null;
        let ws = null;
        let aiWs = null;
        let isVideoEnabled = true;
        let isAudioEnabled = true;
        let isAIPaused = false;
        let isAISpeaking = false;
        let currentSpeaker = null;
        let debugMode = false; // Debug mode toggle
        let chunkRetryAttempts = new Map(); // Track retry attempts for chunks
        let chunkRetryTimeouts = new Map(); // Track retry timeouts for chunks
        let acknowledgedChunks = new Set(); // Track successfully processed chunks
        
        // UI Elements
        const localVideo = document.getElementById('localVideo');
        const remoteVideo = document.getElementById('remoteVideo');
        const aiSphere = document.getElementById('aiSphere');
        const aiIndicator = document.getElementById('aiIndicator');
        const aiStatusText = document.getElementById('aiStatusText');
        const statusIndicator = document.getElementById('statusIndicator');
        const remoteLabel = document.getElementById('remoteLabel');
        const transcriptContent = document.getElementById('transcriptContent');
        const aiResponsePreview = document.getElementById('aiResponsePreview');
        const voiceIndicator = document.getElementById('voiceIndicator');
        const audioLevelDisplay = document.getElementById('audioLevel');
        const voiceStatusDisplay = document.getElementById('voiceStatus');

        // Utility functions
        function updateStatus(message, type = 'info') {
            console.log(`[STATUS] ${message}`);
            statusIndicator.textContent = message;
            statusIndicator.className = 'status-indicator ' + type;
            statusIndicator.style.display = 'block';
            
            if (type === 'success') {
                setTimeout(() => {
                    statusIndicator.style.display = 'none';
                }, 3000);
            }
        }

        function addTranscriptItem(speaker, text, type = 'client') {
            if (userRole !== 'coach') return; // Only show transcript to coaches
            
            const timestamp = new Date().toLocaleTimeString();
            const item = document.createElement('div');
            item.className = `transcript-item ${type}`;
            item.innerHTML = `
                <div class="speaker">${speaker} <span class="timestamp">${timestamp}</span></div>
                <div>${text}</div>
            `;
            
            transcriptContent.appendChild(item);
            transcriptContent.scrollTop = transcriptContent.scrollHeight;
        }

        function updateAIStatus(status, speaking = false) {
            const indicator = aiIndicator;
            const text = aiStatusText;
            const sphere = aiSphere;
            
            // Reset classes
            indicator.className = 'ai-indicator';
            sphere.className = 'ai-sphere';
            
            switch (status) {
                case 'speaking':
                    indicator.classList.add('speaking');
                    sphere.classList.add('speaking');
                    text.textContent = 'AI Speaking';
                    // Start real-time audio analysis for visual effects
                    startAISpeechVisualization();
                    break;
                case 'listening':
                    sphere.classList.add('listening');
                    text.textContent = 'AI Listening';
                    stopAISpeechVisualization();
                    break;
                case 'thinking':
                    sphere.classList.add('thinking');
                    text.textContent = 'AI Processing...';
                    stopAISpeechVisualization();
                    break;
                case 'paused':
                    indicator.classList.add('paused');
                    text.textContent = 'AI Paused - Coach Control';
                    stopAISpeechVisualization();
                    break;
                default:
                    sphere.classList.add('listening');
                    text.textContent = 'AI Ready';
                    stopAISpeechVisualization();
            }
        }

        // Audio visualization for AI speech
        let aiAudioVisualizationInterval = null;
        let speechSynthesisUtterance = null;

        function startAISpeechVisualization() {
            stopAISpeechVisualization(); // Clear any existing interval
            
            // Enhanced visual effect during AI speech
            let intensity = 0;
            let direction = 1;
            
            aiAudioVisualizationInterval = setInterval(() => {
                intensity += direction * 0.1;
                
                if (intensity >= 1) {
                    direction = -1;
                } else if (intensity <= 0.3) {
                    direction = 1;
                }
                
                // Apply dynamic effects to the sphere
                aiSphere.style.filter = `brightness(${1 + intensity * 0.5}) saturate(${1 + intensity * 0.3})`;
                aiSphere.style.transform = `scale(${1 + intensity * 0.1}) rotate(${Date.now() / 20}deg)`;
                
                // Add ripple effect
                if (Math.random() > 0.7) {
                    createAudioRipple();
                }
            }, 100);
        }

        function stopAISpeechVisualization() {
            if (aiAudioVisualizationInterval) {
                clearInterval(aiAudioVisualizationInterval);
                aiAudioVisualizationInterval = null;
            }
            
            // Reset sphere styling
            aiSphere.style.filter = '';
            aiSphere.style.transform = '';
        }

        function createAudioRipple() {
            const ripple = document.createElement('div');
            ripple.style.cssText = `
                position: absolute;
                top: 50%;
                left: 50%;
                width: 120px;
                height: 120px;
                border: 2px solid rgba(23, 162, 184, 0.6);
                border-radius: 50%;
                transform: translate(-50%, -50%);
                pointer-events: none;
                animation: ripple-expand 1.5s ease-out forwards;
            `;
            
            aiSphere.parentElement.appendChild(ripple);
            
            setTimeout(() => {
                ripple.remove();
            }, 1500);
        }

        // Add ripple animation to stylesheet
        if (!document.getElementById('ripple-animation')) {
            const style = document.createElement('style');
            style.id = 'ripple-animation';
            style.textContent = `
                @keyframes ripple-expand {
                    0% {
                        transform: translate(-50%, -50%) scale(0.8);
                        opacity: 1;
                    }
                    100% {
                        transform: translate(-50%, -50%) scale(2);
                        opacity: 0;
                    }
                }
            `;
            document.head.appendChild(style);
        }

        function updateSpeakingIndicator(speaker) {
            // Remove speaking class from all video wrappers
            document.querySelectorAll('.video-wrapper').forEach(wrapper => {
                wrapper.classList.remove('speaking');
            });
            
            // Add speaking class to current speaker
            if (speaker) {
                let wrapper = null;
                if (speaker === 'local') {
                    wrapper = document.getElementById('localVideoWrapper');
                } else if (speaker === 'remote') {
                    wrapper = document.getElementById('remoteVideoWrapper');
                } else if (speaker === 'ai') {
                    wrapper = document.getElementById('aiVideoWrapper');
                }
                
                if (wrapper) {
                    wrapper.classList.add('speaking');
                    console.log(`Speaking indicator activated for: ${speaker}`);
                } else {
                    console.log(`No wrapper found for speaker: ${speaker}`);
                }
            }
            
            currentSpeaker = speaker;
        }

        // Initialize AI WebSocket for coaching session communication
        // Cache audio capabilities to avoid re-testing on reconnections
        let cachedAudioCapabilities = null;
        
        /*
        üìã INSTRUCTIONS FOR GPU CLAUDE - FULL AI CONVERSATION PIPELINE
        
        üöÄ IMPLEMENTED: Full conversation flow with real-time transcription and AI audio responses
        
        FULL CONVERSATION PIPELINE MESSAGE FLOW:
        
        1. üìù REAL-TIME TRANSCRIPTION:
        {
            "type": "client_transcription",
            "sessionId": "session_id_here",
            "text": "I like turtles",
            "confidence": 0.9,
            "timestamp": "2025-06-18T17:30:00.000Z"
        }
        
        2. ü§ñ AI PIPELINE STATUS MESSAGES:
        {
            "type": "ai_thinking",
            "sessionId": "session_id_here", 
            "message": "AI is processing your message..."
        }
        
        {
            "type": "ai_response_text",
            "sessionId": "session_id_here",
            "text": "That's great! Turtles can be very calming..."
        }
        
        {
            "type": "ai_generating_speech",
            "sessionId": "session_id_here",
            "message": "Generating AI speech..."
        }
        
        3. üîä AI AUDIO RESPONSE (MAIN FEATURE):
        {
            "type": "ai_audio_response",
            "sessionId": "session_id_here",
            "audioData": "base64_encoded_mp3_audio",
            "mimeType": "audio/mpeg",
            "duration": 3500,
            "text": "That's great! Turtles can be very calming...",
            "voice_id": "EXAVITQu4vr4xnSDxMaL",
            "timestamp": "2025-06-18T17:30:00.000Z"
        }
        
        4. ‚ùå ERROR HANDLING:
        {
            "type": "ai_error",
            "message": "OpenAI processing failed"
        }
        
        {
            "type": "ai_speech_error", 
            "message": "TTS generation failed",
            "fallback_text": "AI response text for browser TTS"
        }
        
        {
            "type": "ai_pipeline_error",
            "message": "Complete AI pipeline failure"
        }
        
        EXPECTED USER EXPERIENCE:
        1. üé§ User says "I like turtles"
        2. üìù CPU displays: "Client: I like turtles" 
        3. ü§ñ CPU shows: "AI is thinking..."
        4. üìù CPU displays: "AI: That's wonderful! Turtles are..."
        5. üîä CPU plays AI voice speaking the response
        
        BEST PRACTICES FOR GPU:
        - Send client_transcription for real-time display
        - Use ai_audio_response as primary audio delivery method  
        - Include text with audio for transcript display
        - Send status messages for better UX
        - Use fallback_text in ai_speech_error for browser TTS
        - Always include sessionId and timestamps
        */
        
        // Enhanced MediaRecorder testing with better fallbacks
        async function detectAudioCapabilities() {
            // Return cached results if we already tested
            if (cachedAudioCapabilities) {
                console.log('[AUDIO] üéØ Using cached audio capabilities (already tested)');
                console.log('[AUDIO] üéØ Cached preferred format:', cachedAudioCapabilities.preferred_format);
                return cachedAudioCapabilities;
            }
            
            console.log('[AUDIO] üß™ Enhanced MediaRecorder testing with fallbacks...');
            
            // Check environment first
            if (location.protocol !== 'https:' && location.hostname !== 'localhost' && location.hostname !== '127.0.0.1') {
                console.error('[AUDIO] ‚ùå MediaRecorder requires HTTPS or localhost');
                throw new Error('MediaRecorder requires secure context (HTTPS)');
            }
            
            // Check microphone permissions
            try {
                const permissions = await navigator.permissions.query({name: 'microphone'});
                console.log('[AUDIO] Microphone permission status:', permissions.state);
                if (permissions.state === 'denied') {
                    throw new Error('Microphone permission denied');
                }
            } catch (permError) {
                console.log('[AUDIO] Could not check permissions (may be unsupported):', permError.message);
            }
            
            // Enhanced format list with broader fallbacks
            const formatTests = [
                { format: null, name: 'browser-default' },                    // Let browser choose (most reliable)
                { format: 'audio/webm', name: 'webm' },                      // Usually works, Whisper supported
                { format: 'audio/webm;codecs=opus', name: 'webm-opus' },     // Explicit codec
                { format: 'audio/mp4', name: 'mp4' },                        // Whisper likes, but often fails
                { format: 'audio/wav', name: 'wav' },                        // Whisper's favorite, rarely supported
                { format: 'audio/ogg', name: 'ogg' },                        // Firefox fallback
                { format: 'audio/ogg;codecs=opus', name: 'ogg-opus' }        // Firefox with codec
            ];
            
            const workingFormats = [];
            let preferredFormat = null;
            
            // Progressive format testing - test one by one and use first working
            for (const test of formatTests) {
                const { format, name } = test;
                console.log(`[AUDIO] üß™ Testing ${name} (${format || 'default'})...`);
                
                // First check isTypeSupported (quick filter) - but don't fail if browser doesn't support this check
                if (format) {
                    try {
                        if (!MediaRecorder.isTypeSupported(format)) {
                            console.log(`[AUDIO] ‚ùå ${name} - isTypeSupported says no`);
                            continue;
                        }
                    } catch (supportError) {
                        console.log(`[AUDIO] ‚ö†Ô∏è ${name} - isTypeSupported check failed, trying anyway:`, supportError.message);
                    }
                }
                
                // REAL TEST: Try to actually create and start MediaRecorder
                try {
                    // Ensure we have a valid audio stream
                    if (!localStream || localStream.getAudioTracks().length === 0) {
                        console.log(`[AUDIO] ‚ùå No audio stream available for testing`);
                        throw new Error('No audio stream available');
                    }
                    
                    // Create AUDIO-ONLY stream for testing
                    const audioOnlyStream = new MediaStream();
                    const audioTracks = localStream.getAudioTracks();
                    if (audioTracks.length === 0) {
                        throw new Error('No audio tracks in stream');
                    }
                    
                    audioTracks.forEach(track => {
                        audioOnlyStream.addTrack(track);
                    });
                    
                    console.log(`[AUDIO] Testing with ${audioTracks.length} audio tracks`);
                    
                    const testOptions = format ? { mimeType: format } : {};
                    const testRecorder = new MediaRecorder(audioOnlyStream, testOptions);
                    
                    // Test if we can actually start recording
                    const actualMimeType = await new Promise((resolve, reject) => {
                        let testStarted = false;
                        let timeoutId = null;
                        
                        const cleanup = () => {
                            if (timeoutId) clearTimeout(timeoutId);
                        };
                        
                        testRecorder.onstart = () => {
                            testStarted = true;
                            console.log(`[AUDIO] ‚úÖ ${name} - MediaRecorder.start() SUCCESS!`);
                            setTimeout(() => {
                                if (testRecorder.state === 'recording') {
                                    testRecorder.stop();
                                }
                            }, 100); // Stop very quickly
                        };
                        
                        testRecorder.onstop = () => {
                            cleanup();
                            if (testStarted) {
                                resolve(testRecorder.mimeType);
                            } else {
                                reject(new Error('Recording never started'));
                            }
                        };
                        
                        testRecorder.onerror = (event) => {
                            cleanup();
                            reject(new Error(`MediaRecorder error: ${event.error || 'Unknown error'}`));
                        };
                        
                        // Try to start recording
                        try {
                            testRecorder.start(100); // Very short test chunk
                        } catch (startError) {
                            cleanup();
                            reject(startError);
                        }
                        
                        // Timeout after 3 seconds
                        timeoutId = setTimeout(() => {
                            if (!testStarted) {
                                reject(new Error('Recording start timeout'));
                            }
                        }, 3000);
                    });
                    
                    // If we get here, it works!
                    workingFormats.push({
                        requestedFormat: format,
                        actualFormat: actualMimeType,
                        name: name
                    });
                    
                    if (!preferredFormat) {
                        preferredFormat = actualMimeType;
                        console.log(`[AUDIO] üéØ ${name} WORKS! Using as preferred format: ${actualMimeType}`);
                        // Break early - use first working format
                        break;
                    }
                    
                } catch (error) {
                    console.log(`[AUDIO] ‚ùå ${name} FAILED:`, error.message);
                    // Continue to next format
                }
            }
            
            // Browser detection for context
            let browserInfo = 'unknown';
            const userAgent = navigator.userAgent.toLowerCase();
            if (userAgent.includes('chrome') && !userAgent.includes('edg')) {
                browserInfo = userAgent.includes('cros') ? 'chrome-os' : 'chrome';
            } else if (userAgent.includes('safari') && !userAgent.includes('chrome')) {
                browserInfo = 'safari';
            } else if (userAgent.includes('firefox')) {
                browserInfo = 'firefox';
            } else if (userAgent.includes('edg')) {
                browserInfo = 'edge';
            }
            
            console.log(`[AUDIO] üéØ Browser: ${browserInfo}`);
            console.log(`[AUDIO] üéØ Preferred (first working): ${preferredFormat}`);
            console.log(`[AUDIO] üéØ All working formats:`, workingFormats);
            
            // Handle case where no formats work
            if (!preferredFormat || workingFormats.length === 0) {
                console.error('[AUDIO] ‚ùå No working MediaRecorder formats found!');
                console.log('[AUDIO] ü©∫ Diagnostics:');
                console.log('  - Protocol:', location.protocol);
                console.log('  - Hostname:', location.hostname);
                console.log('  - MediaRecorder available:', typeof MediaRecorder !== 'undefined');
                console.log('  - Audio tracks:', localStream ? localStream.getAudioTracks().length : 'No stream');
                console.log('  - Browser:', browserInfo);
                
                // Try one last fallback with minimal options
                try {
                    console.log('[AUDIO] üîÑ Attempting final fallback...');
                    const fallbackRecorder = new MediaRecorder(localStream);
                    const fallbackFormat = fallbackRecorder.mimeType;
                    console.log('[AUDIO] ‚úÖ Final fallback works:', fallbackFormat);
                    
                    cachedAudioCapabilities = {
                        preferred_format: fallbackFormat,
                        working_formats: [{ requestedFormat: null, actualFormat: fallbackFormat, name: 'final-fallback' }],
                        browser_info: browserInfo,
                        chunk_duration: 3000,
                        sample_rate: 48000,
                        tested_real_recording: false,
                        fallback_used: true
                    };
                    
                    return cachedAudioCapabilities;
                    
                } catch (finalError) {
                    console.error('[AUDIO] ‚ùå Final fallback also failed:', finalError.message);
                    throw new Error('No working MediaRecorder formats found during testing');
                }
            }
            
            // Cache results for future use (avoid re-testing on reconnections)
            cachedAudioCapabilities = {
                preferred_format: preferredFormat,
                working_formats: workingFormats,
                browser_info: browserInfo,
                chunk_duration: 3000,
                sample_rate: 48000,
                tested_real_recording: true
            };
            
            console.log('[AUDIO] üéØ Audio capabilities cached for future reconnections');
            
            return cachedAudioCapabilities;
        }

        // Global connection management variables
        let connectionState = 'disconnected';
        let missedPings = 0;
        let lastPingTime = 0;
        let pingInterval = null;
        let ttsKeepAliveInterval = null; // Additional keepalive during TTS
        let connectionTimeout = null;
        let reconnectAttempts = 0;
        let maxReconnectAttempts = 5;
        
        function updateConnectionState(newState) {
            console.log('[AI-WS] Connection state:', connectionState, '->', newState);
            connectionState = newState;
            
            // Update UI status based on connection state
            switch (newState) {
                case 'connected':
                    updateStatus('AI system connected', 'success');
                    break;
                case 'reconnecting':
                    updateStatus('AI reconnecting...', 'warning');
                    break;
                case 'disconnected':
                    updateStatus('AI disconnected - running in demo mode', 'warning');
                    break;
            }
        }
        
        function sendPing() {
            if (aiWs && aiWs.readyState === WebSocket.OPEN) {
                lastPingTime = Date.now();
                aiWs.send(JSON.stringify({
                    type: 'ping',
                    sessionId: sessionId,
                    timestamp: lastPingTime
                }));
                console.log('[AI-WS] Sent keep-alive ping #' + (missedPings + 1));
                
                // Set timeout for pong response
                connectionTimeout = setTimeout(() => {
                    missedPings++;
                    console.log('[AI-WS] Ping timeout - missed pings:', missedPings);
                    
                    if (missedPings >= 3) {
                        console.log('[AI-WS] 3 missed pings - connection considered lost');
                        updateConnectionState('disconnected');
                        clearInterval(pingInterval);
                        if (aiWs) {
                            aiWs.close();
                        }
                    }
                }, 10000); // 10 second timeout for pong response
            } else {
                console.log('[AI-WS] Cannot send ping - WebSocket not open');
                clearInterval(pingInterval);
            }
        }
        
        function handlePong() {
            console.log('[AI-WS] Received pong from GPU - connection alive');
            // Reset ping tracking
            missedPings = 0;
            if (connectionTimeout) {
                clearTimeout(connectionTimeout);
                connectionTimeout = null;
            }
            if (connectionState !== 'connected') {
                updateConnectionState('connected');
            }
            // Reset reconnect attempts on successful connection
            reconnectAttempts = 0;
            
            // Send pong acknowledgment back to GPU to confirm we received it
            if (aiWs && aiWs.readyState === WebSocket.OPEN) {
                aiWs.send(JSON.stringify({
                    type: 'pong_ack',
                    sessionId: sessionId,
                    timestamp: Date.now()
                }));
                console.log('[AI-WS] Sent pong acknowledgment to GPU');
            }
        }
        
        function attemptReconnection(reason = 'unknown') {
            if (reconnectAttempts >= maxReconnectAttempts) {
                console.log('[AI-WS] Max reconnection attempts reached, giving up');
                updateConnectionState('disconnected');
                enableDemoMode();
                return;
            }
            
            reconnectAttempts++;
            // Exponential backoff: 1s, 2s, 4s, 8s, 16s
            const delay = Math.min(1000 * Math.pow(2, reconnectAttempts - 1), 30000);
            
            console.log(`[AI-WS] Reconnection attempt ${reconnectAttempts}/${maxReconnectAttempts} in ${delay}ms (reason: ${reason})`);
            updateConnectionState('reconnecting');
            
            setTimeout(() => {
                console.log('[AI-WS] Attempting to reconnect to AI system');
                aiWs = null;
                initializeAIConnection(); // Recursive reconnect
            }, delay);
        }

        function initializeAIConnection() {
            connectionState = 'connecting';
            
            // Connect through CPU server proxy to GPU server
            const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
            const aiWsUrl = `${protocol}//${window.location.host}/ai-session/${sessionId}`;
            
            console.log('[AI-WS] Connecting to GPU server via CPU proxy:', aiWsUrl);
            console.log('[AI-WS] Session ID:', sessionId);
            
            try {
                aiWs = new WebSocket(aiWsUrl);
                console.log('[AI-WS] WebSocket created, initial readyState:', aiWs.readyState);
                
                // Add connection timeout
                const connectionTimeout = setTimeout(() => {
                    if (aiWs && aiWs.readyState === WebSocket.CONNECTING) {
                        console.log('[AI-WS] Connection timeout - still CONNECTING after 10 seconds');
                        console.log('[AI-WS] Closing connection due to timeout');
                        aiWs.close();
                    }
                }, 10000);
                
                // Clear timeout on successful connection
                aiWs.addEventListener('open', () => {
                    clearTimeout(connectionTimeout);
                });
                
            } catch (error) {
                console.error('[AI-WS] Failed to create WebSocket:', error);
                updateStatus('AI connection failed - running in demo mode', 'warning');
                return;
            }
            
            aiWs.onopen = async () => {
                console.log('[AI-WS] GPU connection status: SUCCESS');
                console.log('[AI-WS] WebSocket readyState:', aiWs.readyState);
                updateConnectionState('connected');
                
                // Initialize AI session with audio capabilities
                try {
                    const audioCapabilities = await detectAudioCapabilities();
                    const initMessage = {
                        type: 'init_session',
                        sessionId: sessionId,
                        roomId: roomId,
                        userId: userId,
                        userRole: userRole,
                        audio_capabilities: audioCapabilities
                    };
                    console.log('[AI-WS] Sending to GPU:', JSON.stringify(initMessage));
                    console.log('[AI-WS] Audio capabilities:', audioCapabilities);
                    aiWs.send(JSON.stringify(initMessage));
                } catch (audioError) {
                    console.error('[AI-WS] Audio capabilities detection failed:', audioError.message);
                    
                    // Send initialization without audio capabilities
                    const initMessage = {
                        type: 'init_session',
                        sessionId: sessionId,
                        roomId: roomId,
                        userId: userId,
                        userRole: userRole,
                        audio_capabilities: {
                            preferred_format: null,
                            working_formats: [],
                            browser_info: 'unknown',
                            error: audioError.message,
                            audio_disabled: true
                        }
                    };
                    console.log('[AI-WS] Sending to GPU (audio disabled):', JSON.stringify(initMessage));
                    aiWs.send(JSON.stringify(initMessage));
                    
                    // Update status to inform user
                    updateStatus('Audio recording disabled - text responses only', 'warning');
                }
                
                // Start ping interval
                pingInterval = setInterval(sendPing, 8000); // Ping every 8 seconds (more frequent communication)
                sendPing(); // Send first ping immediately
            };
            
            aiWs.onmessage = async (event) => {
                try {
                    let messageText;
                    
                    // Handle both string and Blob messages
                    if (event.data instanceof Blob) {
                        console.log('[AI-WS] Received Blob message, converting to text');
                        messageText = await event.data.text();
                    } else {
                        messageText = event.data;
                    }
                    
                    const data = JSON.parse(messageText);
                    console.log('[AI-WS] Received from GPU:', JSON.stringify(data));
                    handleAIMessage(data);
                } catch (error) {
                    console.error('[AI-WS] Error parsing GPU message:', error);
                    console.log('[AI-WS] Raw message:', event.data);
                    console.log('[AI-WS] Message type:', typeof event.data);
                    if (event.data instanceof Blob) {
                        console.log('[AI-WS] Blob size:', event.data.size);
                    }
                }
            };
            
            aiWs.onerror = (error) => {
                console.error('[AI-WS] GPU connection error:', error);
                console.log('[AI-WS] Error type:', error.type);
                console.log('[AI-WS] WebSocket readyState at error:', aiWs ? aiWs.readyState : 'null');
                console.log('[AI-WS] Error occurred, but keeping connection for debugging');
                updateStatus('AI connection error - running in demo mode', 'warning');
                // Don't immediately set to null for debugging
                // aiWs = null;
                enableDemoMode();
            };
            
            aiWs.onclose = (event) => {
                console.log('[AI-WS] GPU disconnected, reason:', event.reason);
                console.log('[AI-WS] Close code:', event.code);
                console.log('[AI-WS] Was clean close:', event.wasClean);
                console.log('[AI-WS] ReadyState at close:', aiWs ? aiWs.readyState : 'null');
                
                // Clear any active ping intervals
                if (pingInterval) {
                    clearInterval(pingInterval);
                    pingInterval = null;
                }
                if (ttsKeepAliveInterval) {
                    clearInterval(ttsKeepAliveInterval);
                    ttsKeepAliveInterval = null;
                }
                if (connectionTimeout) {
                    clearTimeout(connectionTimeout);
                    connectionTimeout = null;
                }
                
                // Determine if we should attempt reconnection
                const shouldReconnect = (
                    event.code === 1006 || // Abnormal closure
                    event.code === 1001 || // Going away
                    event.code === 1000    // Normal closure but unexpected
                );
                
                if (shouldReconnect && connectionState !== 'disconnected') {
                    const reason = `code_${event.code}`;
                    console.log('[AI-WS] Unexpected disconnection detected');
                    attemptReconnection(reason);
                } else {
                    console.log('[AI-WS] Clean disconnection or already disconnected');
                    updateConnectionState('disconnected');
                    
                    // Don't immediately set to null for debugging
                    setTimeout(() => {
                        console.log('[AI-WS] Setting aiWs to null after close');
                        aiWs = null;
                    }, 1000);
                    
                    enableDemoMode();
                }
            };
        }
        
        // Enable demo mode when AI connection fails
        function enableDemoMode() {
            console.log('AI Demo Mode enabled');
            // Simulate AI responses for testing
            if (userRole === 'coach') {
                // Show coach controls even in demo mode
                document.getElementById('pauseAIBtn').disabled = false;
            }
            updateAIStatus('demo');
        }

        function handleGPUError(errorData) {
            const { error_code, errorCode, message, sessionId } = errorData;
            const code = error_code || errorCode; // Handle both formats
            
            switch (code) {
                case 'AUDIO_001': // Unsupported audio format
                    console.error('[ERROR] GPU cannot process audio format, attempting fallback');
                    updateStatus('Audio format issue - trying fallback', 'warning');
                    break;
                    
                case 'AUDIO_005': // Chunked processing not enabled
                    console.error('[ERROR] GPU chunked processing not enabled');
                    updateStatus('GPU configuring chunked audio processing...', 'warning');
                    break;
                    
                case 'AUDIO_006': // Empty audio chunk - reduce spam
                    // Only log occasionally to avoid console spam
                    if (Math.random() < 0.1) { // 10% chance to log
                        console.log('[AUDIO] Empty chunk detected by GPU - filtering audio more strictly');
                    }
                    // Don't update status for this common error
                    return; // Skip the full error logging below
                    
                case 'STT_001': // Transcription failed
                    console.error('[ERROR] Speech transcription failed');
                    updateStatus('Speech recognition failed', 'warning');
                    break;
                    
                case 'API_001': // OpenAI rate limit
                    console.error('[ERROR] OpenAI rate limit exceeded');
                    updateStatus('AI service temporarily unavailable', 'warning');
                    break;
                    
                case 'API_002': // ElevenLabs quota exceeded
                    console.error('[ERROR] Voice synthesis quota exceeded');
                    updateStatus('Voice synthesis unavailable - using text responses', 'warning');
                    break;
                    
                default:
                    console.error('[ERROR] Unknown GPU error:', code, message);
                    updateStatus('AI system error: ' + message, 'error');
            }
            
            // Log error for debugging (skip for AUDIO_006)
            if (code !== 'AUDIO_006') {
                console.log('[ERROR] Full error details:', errorData);
            }
        }

        function handleAIMessage(data) {
            console.log('[AI-WS] Processing GPU message type:', data.type);
            console.log('[AI-WS] Full message:', JSON.stringify(data));
            
            switch (data.type) {
                case 'session_ready':
                    updateAIStatus('listening');
                    if (userRole === 'coach') {
                        addTranscriptItem('System', 'AI session initialized and ready', 'ai');
                    }
                    break;
                    
                case 'session_initialized':
                    console.log('[AI-WS] GPU session fully initialized');
                    updateAIStatus('listening');
                    if (data.audio_format_confirmed) {
                        console.log('[AI-WS] GPU confirmed audio format:', data.audio_format_confirmed);
                    }
                    break;
                    
                case 'transcription_update':
                    console.log('[AI-WS] Received transcription update, partial:', data.partial);
                    if (data.transcript) {
                        if (userRole === 'coach') {
                            const label = data.partial ? 'Client (live)' : 'Client';
                            addTranscriptItem(label, data.transcript, 'client');
                        }
                        // Update status to show transcription is working
                        if (!data.partial) {
                            updateAIStatus('thinking');
                        }
                    }
                    if (data.chunk_sequence !== undefined) {
                        console.log('[AI-WS] Processed chunk sequence:', data.chunk_sequence);
                        
                        // Mark chunk as successfully processed and cancel any retries
                        acknowledgedChunks.add(data.chunk_sequence);
                        
                        const retryTimeout = chunkRetryTimeouts.get(data.chunk_sequence);
                        if (retryTimeout) {
                            clearTimeout(retryTimeout);
                            chunkRetryTimeouts.delete(data.chunk_sequence);
                            console.log(`[AI-WS] Cancelled retry for transcribed chunk ${data.chunk_sequence}`);
                        }
                        
                        chunkRetryAttempts.delete(data.chunk_sequence);
                    }
                    break;
                    
                case 'transcription_result':
                    console.log('[AI-WS] üéôÔ∏è TRANSCRIPTION RESULT:', data.transcript);
                    console.log('[AI-WS] Confidence:', data.confidence || 'unknown');
                    console.log('[AI-WS] Processing time:', data.processing_time || 'unknown');
                    
                    if (data.transcript) {
                        // Add to transcript display for coaches
                        if (userRole === 'coach') {
                            addTranscriptItem('Client (STT)', data.transcript, 'client');
                        }
                        
                        // Show transcription result in status for testing
                        updateStatus(`Transcribed: "${data.transcript}"`, 'success');
                        
                        // Update voice indicator to show successful transcription
                        if (voiceStatusDisplay) {
                            voiceStatusDisplay.textContent = `Transcribed: "${data.transcript.substring(0, 20)}..."`;
                            voiceIndicator.className = 'voice-indicator speaking';
                            voiceIndicator.style.display = 'block';
                            
                            // Reset after 3 seconds
                            setTimeout(() => {
                                voiceStatusDisplay.textContent = 'Listening...';
                                voiceIndicator.className = 'voice-indicator';
                            }, 3000);
                        }
                    }
                    
                    if (data.chunk_sequence !== undefined) {
                        acknowledgedChunks.add(data.chunk_sequence);
                        const retryTimeout = chunkRetryTimeouts.get(data.chunk_sequence);
                        if (retryTimeout) {
                            clearTimeout(retryTimeout);
                            chunkRetryTimeouts.delete(data.chunk_sequence);
                        }
                        chunkRetryAttempts.delete(data.chunk_sequence);
                    }
                    break;
                    
                case 'chunk_received':
                    console.log('[AI-WS] GPU confirmed chunk received:', data.chunk_sequence);
                    if (data.processing_status) {
                        console.log('[AI-WS] Chunk processing status:', data.processing_status);
                    }
                    
                    // Mark chunk as acknowledged and cancel any pending retries
                    if (data.chunk_sequence !== undefined) {
                        acknowledgedChunks.add(data.chunk_sequence);
                        
                        // Cancel retry timeout if exists
                        const retryTimeout = chunkRetryTimeouts.get(data.chunk_sequence);
                        if (retryTimeout) {
                            clearTimeout(retryTimeout);
                            chunkRetryTimeouts.delete(data.chunk_sequence);
                            console.log(`[AI-WS] Cancelled retry for acknowledged chunk ${data.chunk_sequence}`);
                        }
                        
                        // Remove from retry attempts tracking
                        chunkRetryAttempts.delete(data.chunk_sequence);
                    }
                    break;
                    
                case 'client_speaking':
                    updateAIStatus('listening');
                    updateSpeakingIndicator('local');
                    if (userRole === 'coach' && data.transcript) {
                        addTranscriptItem('Client', data.transcript, 'client');
                    }
                    break;
                    
                case 'ai_thinking':
                    updateAIStatus('thinking');
                    if (userRole === 'coach' && data.preview) {
                        aiResponsePreview.textContent = data.preview;
                    }
                    break;
                    
                case 'ai_speaking':
                    updateAIStatus('speaking');
                    updateSpeakingIndicator('ai');
                    isAISpeaking = true;
                    if (userRole === 'coach') {
                        addTranscriptItem('AI Assistant', data.text, 'ai');
                        aiResponsePreview.textContent = 'Waiting for client input...';
                    }
                    
                    // Simulate AI audio playback for all users
                    playAIAudio(data.text);
                    break;
                    
                case 'ai_paused':
                    updateAIStatus('paused');
                    isAIPaused = true;
                    if (userRole === 'coach') {
                        addTranscriptItem('System', 'AI paused - Coach has control', 'coach');
                        document.getElementById('pauseAI').style.display = 'none';
                        document.getElementById('resumeAI').style.display = 'inline-flex';
                    }
                    break;
                    
                case 'ai_resumed':
                    updateAIStatus('listening');
                    isAIPaused = false;
                    if (userRole === 'coach') {
                        addTranscriptItem('System', 'AI resumed - Back in control', 'ai');
                        document.getElementById('pauseAI').style.display = 'inline-flex';
                        document.getElementById('resumeAI').style.display = 'none';
                    }
                    break;
                    
                case 'coach_speaking':
                    updateSpeakingIndicator('remote');
                    if (userRole === 'coach' && data.transcript) {
                        addTranscriptItem('Coach', data.transcript, 'coach');
                    }
                    break;
                    
                case 'ai_audio_data':
                    console.log('[AI-WS] üîä RECEIVED ELEVENLABS AUDIO DATA:');
                    console.log('  - Audio Size:', data.audioData?.length || 0, 'characters (base64)');
                    console.log('  - MIME Type:', data.mimeType || 'unknown');
                    console.log('  - Duration:', data.duration || 'unknown', 'ms');
                    console.log('  - Voice ID:', data.voice_id || 'unknown');
                    console.log('  - Text:', data.text ? `"${data.text.substring(0, 50)}..."` : 'no text provided');
                    console.log('  - Timestamp:', data.timestamp || 'unknown');
                    
                    // Stop TTS keepalive since we got the response
                    if (ttsKeepAliveInterval) {
                        clearInterval(ttsKeepAliveInterval);
                        ttsKeepAliveInterval = null;
                        console.log('[AI-WS] üîÑ TTS keepalive stopped - ElevenLabs audio received');
                    }
                    
                    if (data.audioData && data.audioData.length > 0) {
                        // Show status update that AI is about to speak
                        const displayText = data.text || 'Audio response';
                        updateStatus(`AI speaking: "${displayText.substring(0, 30)}..."`, 'success');
                        
                        // Add to transcript with ElevenLabs attribution
                        if (data.text && userRole === 'coach') {
                            addTranscriptItem('AI (ElevenLabs TTS)', data.text, 'ai');
                        }
                        
                        // Play the ElevenLabs audio (MP3 format)
                        console.log('[AI-WS] üéµ Playing ElevenLabs MP3 audio...');
                        playRealAIAudio(data.audioData, data.mimeType || 'audio/mpeg');
                    } else {
                        console.log('[AI-WS] ‚ùå Received ai_audio_data but no audioData provided');
                        updateStatus('Received empty audio data from AI', 'warning');
                        
                        // Fallback to showing text if available
                        if (data.text) {
                            if (userRole === 'coach') {
                                addTranscriptItem('AI (Text Fallback)', data.text, 'ai');
                            }
                            updateStatus(`AI: "${data.text.substring(0, 50)}..."`, 'info');
                        }
                        updateAIStatus('listening');
                    }
                    break;
                    
                case 'ai_finished_speaking':
                    console.log('AI finished speaking, ready for next interaction');
                    updateAIStatus('listening');
                    updateSpeakingIndicator(null);
                    isAISpeaking = false;
                    break;
                    
                case 'client_transcription':
                    console.log('[AI-WS] üìù CLIENT TRANSCRIPTION:', data.text);
                    console.log('  - Confidence:', data.confidence || 'unknown');
                    console.log('  - Timestamp:', data.timestamp || 'unknown');
                    
                    if (data.text) {
                        // Add to transcript display for coaches
                        if (userRole === 'coach') {
                            addTranscriptItem('Client', data.text, 'client');
                        }
                        
                        // Show transcription result in status
                        updateStatus(`You said: "${data.text}"`, 'success');
                        
                        // Update AI status to show processing
                        updateAIStatus('thinking');
                    }
                    break;
                    
                case 'ai_thinking':
                    console.log('[AI-WS] ü§ñ AI THINKING:', data.message);
                    updateAIStatus('thinking');
                    if (userRole === 'coach' && data.message) {
                        if (aiResponsePreview) {
                            aiResponsePreview.textContent = data.message;
                        }
                    }
                    updateStatus(data.message || 'AI is processing...', 'info');
                    break;
                    
                case 'ai_response_text':
                    console.log('[AI-WS] üìù AI TEXT RESPONSE:', data.text ? `"${data.text.substring(0, 100)}..."` : 'no text');
                    
                    if (data.text) {
                        // Add to transcript for coaches
                        if (userRole === 'coach') {
                            addTranscriptItem('AI', data.text, 'ai');
                        }
                        
                        // Update AI response preview
                        if (aiResponsePreview) {
                            aiResponsePreview.textContent = data.text;
                        }
                        
                        // Show status that we got AI response
                        updateStatus(`AI responded: "${data.text.substring(0, 30)}..."`, 'success');
                    }
                    break;
                    
                case 'ai_generating_speech':
                    console.log('[AI-WS] üéôÔ∏è AI GENERATING SPEECH:', data.message);
                    updateAIStatus('thinking');
                    updateStatus(data.message || 'Generating AI speech...', 'info');
                    
                    // Start more frequent keepalive during TTS generation
                    if (ttsKeepAliveInterval) {
                        clearInterval(ttsKeepAliveInterval);
                    }
                    ttsKeepAliveInterval = setInterval(() => {
                        if (aiWs && aiWs.readyState === WebSocket.OPEN) {
                            aiWs.send(JSON.stringify({
                                type: 'tts_keepalive',
                                sessionId: sessionId,
                                timestamp: Date.now()
                            }));
                            console.log('[AI-WS] üîÑ TTS keepalive sent');
                        }
                    }, 2000); // Every 2 seconds during TTS
                    break;
                    
                case 'ai_audio_response':
                    console.log('[AI-WS] üîä AI AUDIO RESPONSE RECEIVED:');
                    console.log('  - Audio size:', data.audioData?.length || 0, 'characters (base64)');
                    console.log('  - MIME Type:', data.mimeType || 'unknown');
                    console.log('  - Duration:', data.duration || 'unknown', 'ms');
                    console.log('  - Voice ID:', data.voice_id || 'unknown');
                    console.log('  - Text:', data.text ? `"${data.text.substring(0, 50)}..."` : 'no text');
                    
                    // Stop TTS keepalive since we got the response
                    if (ttsKeepAliveInterval) {
                        clearInterval(ttsKeepAliveInterval);
                        ttsKeepAliveInterval = null;
                        console.log('[AI-WS] üîÑ TTS keepalive stopped - audio received');
                    }
                    
                    if (data.audioData && data.audioData.length > 0) {
                        // Show status that we're playing AI response
                        updateStatus(`AI speaking: "${(data.text || 'Audio response').substring(0, 30)}..."`, 'success');
                        
                        // Add text to transcript if available
                        if (data.text && userRole === 'coach') {
                            addTranscriptItem('AI (TTS)', data.text, 'ai');
                        }
                        
                        // Play the AI audio response
                        playRealAIAudio(data.audioData, data.mimeType || 'audio/mpeg');
                    } else {
                        console.log('[AI-WS] ‚ùå Received ai_audio_response but no audioData');
                        updateStatus('Received empty audio response from AI', 'warning');
                        
                        // Fallback to text-to-speech if we have text
                        if (data.text) {
                            playAIAudio(data.text);
                        }
                    }
                    break;
                    
                case 'tts_failed':
                    console.log('[AI-WS] ‚ö†Ô∏è TTS FAILED - falling back to text response');
                    console.log('  - Error:', data.error || 'unknown');
                    console.log('  - Text:', data.text ? `"${data.text.substring(0, 50)}..."` : 'no text');
                    
                    // Stop TTS keepalive since TTS failed
                    if (ttsKeepAliveInterval) {
                        clearInterval(ttsKeepAliveInterval);
                        ttsKeepAliveInterval = null;
                        console.log('[AI-WS] üîÑ TTS keepalive stopped - TTS failed');
                    }
                    
                    updateStatus('AI audio failed - using text response', 'warning');
                    updateAIStatus('listening');
                    
                    // Show text response instead
                    if (data.text) {
                        if (userRole === 'coach') {
                            addTranscriptItem('AI (Text)', data.text, 'ai');
                        }
                        updateStatus(`AI: "${data.text.substring(0, 50)}..."`, 'info');
                    }
                    break;
                    
                case 'websocket_disconnected':
                    console.log('[AI-WS] ‚ö†Ô∏è GPU reports WebSocket disconnected during TTS');
                    console.log('  - Reason:', data.reason || 'unknown');
                    console.log('  - Text:', data.text ? `"${data.text.substring(0, 50)}..."` : 'no text');
                    
                    updateStatus('Connection lost during AI response - reconnecting...', 'warning');
                    
                    // Show text fallback if available
                    if (data.text) {
                        if (userRole === 'coach') {
                            addTranscriptItem('AI (Text Fallback)', data.text, 'ai');
                        }
                        updateStatus(`AI: "${data.text.substring(0, 50)}..."`, 'info');
                    }
                    
                    // Force reconnection attempt
                    attemptReconnection('tts_disconnection');
                    break;
                    
                case 'ai_error':
                    console.error('[AI-WS] ‚ùå AI ERROR:', data.message || 'OpenAI processing failed');
                    updateStatus('AI processing failed - please try again', 'error');
                    updateAIStatus('listening');
                    break;
                    
                case 'ai_speech_error':
                    console.error('[AI-WS] ‚ùå AI SPEECH ERROR:', data.message || 'TTS generation failed');
                    updateStatus('AI speech generation failed - using text fallback', 'warning');
                    
                    // Use fallback text if available
                    if (data.fallback_text) {
                        console.log('[AI-WS] Using fallback text for speech synthesis');
                        playAIAudio(data.fallback_text);
                        
                        if (userRole === 'coach') {
                            addTranscriptItem('AI (Fallback)', data.fallback_text, 'ai');
                        }
                    } else {
                        updateAIStatus('listening');
                    }
                    break;
                    
                case 'ai_pipeline_error':
                    console.error('[AI-WS] ‚ùå AI PIPELINE ERROR:', data.message || 'Complete AI pipeline failure');
                    updateStatus('AI system error - please try speaking again', 'error');
                    updateAIStatus('listening');
                    break;
                
                case 'openai_response':
                    console.log('[AI-WS] ü§ñ OPENAI RESPONSE RECEIVED:');
                    console.log('  - Text:', data.text ? `"${data.text.substring(0, 100)}..."` : 'no text');
                    console.log('  - Model:', data.model || 'unknown');
                    console.log('  - Tokens:', data.tokens || 'unknown');
                    
                    if (data.text) {
                        // Add to transcript for coaches
                        if (userRole === 'coach') {
                            addTranscriptItem('AI Response (OpenAI)', data.text, 'ai');
                        }
                        
                        // Update AI response preview
                        if (aiResponsePreview) {
                            aiResponsePreview.textContent = data.text;
                        }
                        
                        // Show status that we got OpenAI response
                        updateStatus(`OpenAI responded: "${data.text.substring(0, 30)}..."`, 'success');
                        updateAIStatus('thinking'); // Now processing TTS
                    }
                    break;
                    
                case 'pong':
                    handlePong();
                    break;
                    
                case 'error':
                    console.error('[AI-WS] Received error from GPU:', data.error_code, data.message);
                    handleGPUError(data);
                    break;
                    
                default:
                    console.log(`[AI-WS] ‚ö†Ô∏è Unhandled message type: "${data.type}"`, data);
                    break;
            }
        }

        // Play real AI audio from GPU server (ElevenLabs TTS)
        function playRealAIAudio(base64Audio, mimeType) {
            try {
                console.log('[AI-AUDIO] üéµ Processing ElevenLabs TTS audio playback:');
                console.log('  - Base64 length:', base64Audio.length, 'characters');
                console.log('  - MIME type:', mimeType);
                console.log('  - Expected format: MP3 from ElevenLabs API');
                
                // Validate base64 input
                if (!base64Audio || base64Audio.length < 100) {
                    throw new Error(`Invalid base64 audio data: ${base64Audio?.length || 0} characters`);
                }
                
                // Convert base64 to blob
                console.log('[AI-AUDIO] Converting base64 to audio blob...');
                const audioData = atob(base64Audio);
                const bytes = new Uint8Array(audioData.length);
                for (let i = 0; i < audioData.length; i++) {
                    bytes[i] = audioData.charCodeAt(i);
                }
                const audioBlob = new Blob([bytes], { type: mimeType });
                console.log('[AI-AUDIO] Audio blob created, size:', audioBlob.size, 'bytes');
                
                // Create audio element and play
                const audioUrl = URL.createObjectURL(audioBlob);
                const audio = new Audio(audioUrl);
                
                // Set up comprehensive audio event handlers
                audio.onloadstart = () => {
                    console.log('[AI-AUDIO] ‚úÖ ElevenLabs audio loading started');
                    updateSpeakingIndicator('ai');
                    updateAIStatus('speaking');
                    isAISpeaking = true;
                    
                    // Visual feedback that AI is about to speak
                    if (voiceStatusDisplay) {
                        voiceStatusDisplay.textContent = 'AI Loading Audio...';
                        voiceIndicator.className = 'voice-indicator speaking';
                        voiceIndicator.style.display = 'block';
                    }
                };
                
                audio.oncanplay = () => {
                    console.log('[AI-AUDIO] ‚úÖ AI audio can start playing');
                };
                
                audio.onplay = () => {
                    console.log('[AI-AUDIO] ‚úÖ ElevenLabs audio playback started successfully');
                    
                    // Update visual indicators that AI is actively speaking
                    if (voiceStatusDisplay) {
                        voiceStatusDisplay.textContent = 'AI Speaking...';
                        voiceIndicator.className = 'voice-indicator speaking';
                        voiceIndicator.style.display = 'block';
                    }
                };
                
                audio.onended = () => {
                    console.log('[AI-AUDIO] ‚úÖ ElevenLabs audio playback completed');
                    updateSpeakingIndicator(null);
                    updateAIStatus('listening');
                    isAISpeaking = false;
                    URL.revokeObjectURL(audioUrl); // Clean up
                    
                    // Reset visual indicators to listening state
                    if (voiceStatusDisplay) {
                        voiceStatusDisplay.textContent = 'Listening...';
                        voiceIndicator.className = 'voice-indicator';
                        voiceIndicator.style.display = 'none';
                    }
                };
                
                audio.onerror = (error) => {
                    console.error('[AI-AUDIO] ‚ùå ElevenLabs audio playback error:', error);
                    console.error('[AI-AUDIO] Audio element error details:', {
                        error: audio.error,
                        networkState: audio.networkState,
                        readyState: audio.readyState,
                        src: audioUrl.substring(0, 50) + '...',
                        mimeType: mimeType,
                        blobSize: audioBlob.size
                    });
                    updateSpeakingIndicator(null);
                    updateAIStatus('listening');
                    isAISpeaking = false;
                    URL.revokeObjectURL(audioUrl); // Clean up
                    updateStatus('ElevenLabs audio playback failed', 'error');
                    
                    // Reset visual indicators
                    if (voiceStatusDisplay) {
                        voiceStatusDisplay.textContent = 'Audio Error - Listening...';
                        voiceIndicator.className = 'voice-indicator noise';
                        voiceIndicator.style.display = 'block';
                        
                        // Reset to normal after 3 seconds
                        setTimeout(() => {
                            voiceStatusDisplay.textContent = 'Listening...';
                            voiceIndicator.className = 'voice-indicator';
                            voiceIndicator.style.display = 'none';
                        }, 3000);
                    }
                };
                
                audio.onpause = () => {
                    console.log('[AI-AUDIO] AI audio paused');
                };
                
                audio.onstalled = () => {
                    console.log('[AI-AUDIO] ‚ö†Ô∏è AI audio stalled');
                };
                
                audio.onwaiting = () => {
                    console.log('[AI-AUDIO] ‚ö†Ô∏è AI audio waiting for data');
                };
                
                // Attempt to play the audio
                console.log('[AI-AUDIO] Attempting to play audio...');
                audio.play().then(() => {
                    console.log('[AI-AUDIO] ‚úÖ Audio play() promise resolved');
                }).catch(error => {
                    console.error('[AI-AUDIO] ‚ùå Audio play() promise rejected:', error);
                    console.error('[AI-AUDIO] Play error details:', {
                        name: error.name,
                        message: error.message,
                        code: error.code
                    });
                    updateAIStatus('listening');
                    isAISpeaking = false;
                    updateStatus(`AI audio play failed: ${error.message}`, 'error');
                });
                
            } catch (error) {
                console.error('[AI-AUDIO] ‚ùå Error processing AI audio:', error);
                console.error('[AI-AUDIO] Error details:', {
                    name: error.name,
                    message: error.message,
                    stack: error.stack?.substring(0, 200)
                });
                updateAIStatus('listening');
                isAISpeaking = false;
                updateStatus(`AI audio processing failed: ${error.message}`, 'error');
            }
        }

        function playAIAudio(text) {
            // Fallback to Web Speech API for text-only responses
            // Real audio is now handled by playRealAIAudio function
            if ('speechSynthesis' in window) {
                const utterance = new SpeechSynthesisUtterance(text);
                utterance.rate = 0.9;
                utterance.pitch = 1.1;
                utterance.volume = 0.8;
                
                // Store reference for potential cancellation
                speechSynthesisUtterance = utterance;
                
                utterance.onstart = () => {
                    console.log('AI speech started');
                    updateSpeakingIndicator('ai');
                    updateAIStatus('speaking');
                };
                
                utterance.onend = () => {
                    console.log('AI speech ended');
                    updateSpeakingIndicator(null);
                    updateAIStatus('listening');
                    speechSynthesisUtterance = null;
                    isAISpeaking = false;
                };
                
                utterance.onerror = (event) => {
                    console.error('Speech synthesis error:', event);
                    updateSpeakingIndicator(null);
                    updateAIStatus('listening');
                    speechSynthesisUtterance = null;
                };
                
                // Enhanced visualization during speech
                utterance.onboundary = (event) => {
                    // Create visual effects at word boundaries
                    if (event.name === 'word') {
                        createAudioRipple();
                    }
                };
                
                speechSynthesis.speak(utterance);
            }
        }

        // AI Control functions (Coach only)
        function pauseAI() {
            // Stop any current AI speech immediately
            if (speechSynthesis.speaking) {
                speechSynthesis.cancel();
            }
            stopAISpeechVisualization();
            
            if (aiWs && aiWs.readyState === WebSocket.OPEN) {
                aiWs.send(JSON.stringify({
                    type: 'pause_ai',
                    sessionId: sessionId,
                    coachId: userId
                }));
            }
            
            updateAIStatus('paused');
        }

        function resumeAI() {
            if (aiWs && aiWs.readyState === WebSocket.OPEN) {
                aiWs.send(JSON.stringify({
                    type: 'resume_ai',
                    sessionId: sessionId,
                    coachId: userId
                }));
            }
        }

        // Initialize media and connections
        async function initializeMedia() {
            console.log('Initializing media...');
            try {
                updateStatus('Accessing camera and microphone...');
                
                const constraints = {
                    video: { 
                        width: { ideal: 640, max: 1280 },
                        height: { ideal: 480, max: 720 },
                        facingMode: 'user'
                    },
                    audio: {
                        echoCancellation: true,
                        noiseSuppression: true,
                        autoGainControl: true
                    }
                };
                
                localStream = await navigator.mediaDevices.getUserMedia(constraints);
                localVideo.srcObject = localStream;
                
                console.log('‚úÖ Media initialized successfully');
                console.log('Local stream audio tracks:', localStream.getAudioTracks().length);
                console.log('Local stream video tracks:', localStream.getVideoTracks().length);
                
                updateStatus('Media ready', 'success');
                setupControls();
                initializeWebRTCConnection();
                initializeAIConnection();
                
            } catch (err) {
                console.error('Media initialization error:', err);
                updateStatus('Media error: ' + err.message, 'error');
                
                // Still try to connect for receive-only
                initializeWebRTCConnection();
                initializeAIConnection();
            }
        }

        function initializeWebRTCConnection() {
            // WebRTC connection logic (simplified for this example)
            // This would integrate with your existing WebRTC implementation
            updateStatus('Connecting to video call...', 'info');
            
            // Simulate connection for demo
            setTimeout(() => {
                updateStatus('Video call connected', 'success');
                remoteLabel.textContent = userRole === 'coach' ? 'Client Connected' : 'Coach Connected';
            }, 2000);
        }

        function setupControls() {
            // Video toggle
            document.getElementById('toggleVideo').onclick = () => {
                isVideoEnabled = !isVideoEnabled;
                localStream.getVideoTracks().forEach(track => {
                    track.enabled = isVideoEnabled;
                });
                
                const btn = document.getElementById('toggleVideo');
                btn.classList.toggle('active', !isVideoEnabled);
                btn.innerHTML = isVideoEnabled ? 'üìπ Video' : 'üìπ Video Off';
            };
            
            // Audio toggle
            document.getElementById('toggleAudio').onclick = () => {
                isAudioEnabled = !isAudioEnabled;
                localStream.getAudioTracks().forEach(track => {
                    track.enabled = isAudioEnabled;
                });
                
                const btn = document.getElementById('toggleAudio');
                btn.classList.toggle('active', !isAudioEnabled);
                btn.innerHTML = isAudioEnabled ? 'üé§ Audio' : 'üé§ Muted';
            };
            
            // Coach controls
            if (userRole === 'coach') {
                document.getElementById('pauseAI').onclick = pauseAI;
                document.getElementById('resumeAI').onclick = resumeAI;
                document.getElementById('quickPause').onclick = pauseAI;
                
                // Test button for sending "I like turtles" to GPU
                document.getElementById('testAudio').onclick = () => {
                    if (aiWs && aiWs.readyState === WebSocket.OPEN) {
                        const testMessage = {
                            type: 'test_transcription',
                            sessionId: sessionId,
                            test_text: 'I like turtles',
                            timestamp: Date.now()
                        };
                        console.log('[AI-WS] üß™ Sending test transcription to GPU');
                        aiWs.send(JSON.stringify(testMessage));
                        updateStatus('Sent test transcription to GPU', 'success');
                    } else {
                        updateStatus('GPU not connected', 'error');
                    }
                };
                
                // Tab switching
                document.querySelectorAll('.tab').forEach(tab => {
                    tab.onclick = () => {
                        document.querySelectorAll('.tab').forEach(t => t.classList.remove('active'));
                        tab.classList.add('active');
                        
                        // Switch transcript view (placeholder for now)
                        console.log('Switched to tab:', tab.dataset.tab);
                    };
                });
            }
        }

        // Track if audio detection is already set up
        let audioDetectionSetup = false;
        
        // Audio detection for speaking indicators
        async function setupAudioDetection() {
            if (audioDetectionSetup) {
                console.log('[AUDIO] üéØ Audio detection already setup, skipping...');
                return;
            }
            
            console.log('Setting up audio detection...');
            if (localStream) {
                console.log('‚úÖ Local stream available, initializing audio detection');
                try {
                    const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                    const analyser = audioContext.createAnalyser();
                    const microphone = audioContext.createMediaStreamSource(localStream);
                    const dataArray = new Uint8Array(analyser.frequencyBinCount);
                    
                    // Use REAL tested audio capabilities
                    const audioCapabilities = await detectAudioCapabilities();
                    const workingFormats = audioCapabilities.working_formats;
                    
                    console.log('[AUDIO] Using REAL tested formats:', workingFormats);
                    
                    // Set up chunked audio recording for real speech-to-text
                    let mediaRecorder = null;
                    let audioChunks = [];
                    let currentMimeType = null;
                    let chunkSequence = 0;
                    let isRecording = false;
                    let chunkDuration = 3000; // 3 seconds per chunk
                    let recordingStartTime = 0;
                    let overlapBuffer = []; // Store last chunk for overlap
                    
                    // Use ALREADY TESTED working formats (no more guessing!)
                    async function initializeMediaRecorder() {
                        if (workingFormats.length === 0) {
                            throw new Error('No working MediaRecorder formats found during testing');
                        }
                        
                        // Create AUDIO-ONLY stream from local stream
                        const audioOnlyStream = new MediaStream();
                        localStream.getAudioTracks().forEach(track => {
                            audioOnlyStream.addTrack(track);
                        });
                        console.log(`[AUDIO] üéµ Created audio-only stream with ${audioOnlyStream.getAudioTracks().length} audio tracks`);
                        
                        // Use the first (preferred) working format
                        const bestFormat = workingFormats[0];
                        console.log(`[AUDIO] üéØ Using PROVEN working format: ${bestFormat.name} (${bestFormat.actualFormat})`);
                        
                        try {
                            const options = bestFormat.requestedFormat ? { mimeType: bestFormat.requestedFormat } : {};
                            mediaRecorder = new MediaRecorder(audioOnlyStream, options);
                            currentMimeType = mediaRecorder.mimeType;
                            
                            console.log(`[AUDIO] ‚úÖ MediaRecorder created with tested format`);
                            console.log(`[AUDIO] ‚úÖ Requested: ${bestFormat.requestedFormat || 'browser-default'}`);
                            console.log(`[AUDIO] ‚úÖ Actual: ${currentMimeType}`);
                            
                            return mediaRecorder;
                            
                        } catch (error) {
                            console.error(`[AUDIO] ‚ùå IMPOSSIBLE! Tested format ${bestFormat.name} failed:`, error);
                            
                            // This should never happen since we tested it, but try next format as safety
                            if (workingFormats.length > 1) {
                                console.log('[AUDIO] üîÑ Trying next tested format as emergency fallback...');
                                const secondFormat = workingFormats[1];
                                const options2 = secondFormat.requestedFormat ? { mimeType: secondFormat.requestedFormat } : {};
                                mediaRecorder = new MediaRecorder(localStream, options2);
                                currentMimeType = mediaRecorder.mimeType;
                                console.log(`[AUDIO] ‚úÖ Emergency fallback to: ${currentMimeType}`);
                                return mediaRecorder;
                            }
                            
                            throw error;
                        }
                    }
                    
                    try {
                        await initializeMediaRecorder();
                        console.log('üé§ MediaRecorder initialized successfully for audio detection');
                        
                        function setupMediaRecorderHandlers() {
                            if (!mediaRecorder) {
                                console.error('[AUDIO] Cannot setup handlers - MediaRecorder is null');
                                return;
                            }
                            
                            // Clear any existing handlers
                            mediaRecorder.ondataavailable = null;
                            mediaRecorder.onstop = null;
                            mediaRecorder.onerror = null;
                            
                            mediaRecorder.ondataavailable = (event) => {
                                // Check recording duration to avoid very short recordings
                                const recordingDuration = Date.now() - recordingStartTime;
                                if (recordingDuration < 500) { // Less than 0.5 seconds
                                    console.log(`[AUDIO] Recording too short (${recordingDuration}ms), skipping chunk`);
                                    return;
                                }
                                
                                // Skip chunks that are too small or empty
                                if (event.data.size === 0) {
                                    console.log(`[AUDIO] Skipping empty chunk (0 bytes)`);
                                    return;
                                }
                                
                                if (event.data.size < 1000) {
                                    console.log(`[AUDIO] Skipping tiny chunk, size: ${event.data.size} bytes (< 1KB minimum)`);
                                    return;
                                }
                                
                                console.log(`[AUDIO] Chunk data available, size: ${event.data.size} bytes, duration: ${recordingDuration}ms`);
                                
                                // Create chunk with overlap if we have previous data
                                let chunkData = [event.data];
                                if (overlapBuffer.length > 0) {
                                    // Add 200ms overlap from previous chunk
                                    chunkData = [...overlapBuffer, event.data];
                                    console.log('[AUDIO] Added 200ms overlap from previous chunk');
                                }
                                
                                const chunkBlob = new Blob(chunkData, { type: currentMimeType });
                                
                                // Validate final blob size before sending
                                if (chunkBlob.size < 1000) {
                                    console.log(`[AUDIO] Final blob too small (${chunkBlob.size} bytes), skipping`);
                                    return;
                                }
                                
                                // Send chunk immediately for real-time processing
                                console.log(`[AUDIO] üì¶ Sending chunk #${chunkSequence} (${chunkBlob.size} bytes)`);
                                sendAudioChunk(chunkBlob, false);
                                
                                // Store last part for next chunk's overlap (simulate 200ms)
                                overlapBuffer = [event.data];
                            };
                            
                            mediaRecorder.onstop = () => {
                                // Store final count BEFORE reset
                                const sessionChunkCount = chunkSequence;
                                console.log(`[AUDIO] üìä Recording session complete - sent ${sessionChunkCount} chunks total`);
                                console.log('[AUDIO] Recording stopped, sending final chunk if needed');
                                isRecording = false;
                                
                                // Send final marker to indicate end of speech
                                if (aiWs && aiWs.readyState === WebSocket.OPEN) {
                                    const finalMessage = {
                                        type: 'audio_chunk',
                                        sessionId: sessionId,
                                        chunk_sequence: chunkSequence,
                                        total_chunks: chunkSequence,
                                        audioData: null, // No data, just end marker
                                        mimeType: currentMimeType,
                                        duration: 0,
                                        overlap: 0,
                                        timestamp: Date.now()
                                    };
                                    console.log(`[AUDIO] üì§ Sending end-of-speech marker (session had ${sessionChunkCount} chunks)`);
                                    aiWs.send(JSON.stringify(finalMessage));
                                }
                                
                                // Reset for next recording session
                                chunkSequence = 0;
                                overlapBuffer = [];
                            };
                            
                            mediaRecorder.onerror = (event) => {
                                console.error('[AUDIO] MediaRecorder error:', event.error);
                                // Send error to GPU for debugging
                                if (aiWs && aiWs.readyState === WebSocket.OPEN) {
                                    aiWs.send(JSON.stringify({
                                        type: 'error',
                                        error_code: 'AUDIO_002',
                                        message: 'MediaRecorder error: ' + event.error,
                                        sessionId: sessionId,
                                        timestamp: Date.now()
                                    }));
                                }
                            };
                            
                            console.log('[AUDIO] ‚úÖ MediaRecorder event handlers setup complete');
                        }
                        
                        function sendAudioChunk(audioBlob, isLastChunk = false, retryAttempt = 0) {
                            // Check if chunk was already acknowledged (in case of race conditions)
                            if (acknowledgedChunks.has(chunkSequence)) {
                                console.log(`[AUDIO] Chunk ${chunkSequence} already acknowledged, skipping send`);
                                return;
                            }
                            
                            // Additional check: Don't send if this chunk sequence was already processed
                            if (chunkSequence === 0 && acknowledgedChunks.size > 0) {
                                console.log(`[AUDIO] Skipping duplicate chunk 0 - session already processed`);
                                return;
                            }
                            
                            // Validate audio chunk size
                            if (audioBlob.size === 0) {
                                console.log('[AUDIO] Empty chunk detected, skipping send');
                                return;
                            }
                            
                            if (audioBlob.size < 1000) { // Must be at least 1KB
                                console.log(`[AUDIO] Chunk too small (${audioBlob.size} bytes), skipping - no meaningful audio`);
                                return;
                            }
                            
                            if (audioBlob.size > 5 * 1024 * 1024) { // 5MB limit per chunk
                                console.error('[AUDIO] Chunk too large:', audioBlob.size, 'bytes');
                                return;
                            }
                            
                            // Convert to base64 for transmission
                            const reader = new FileReader();
                            reader.onload = () => {
                                const base64Audio = reader.result.split(',')[1];
                                
                                // Final validation - check base64 data is meaningful
                                if (!base64Audio || base64Audio.length < 100) {
                                    console.log(`[AUDIO] Base64 data too short (${base64Audio ? base64Audio.length : 0} chars), skipping - likely empty audio`);
                                    return;
                                }
                                
                                if (aiWs && aiWs.readyState === WebSocket.OPEN) {
                                    const chunkMessage = {
                                        type: 'audio_chunk',
                                        sessionId: sessionId,
                                        chunk_sequence: chunkSequence,
                                        total_chunks: isLastChunk ? chunkSequence + 1 : null,
                                        audioData: base64Audio,
                                        mimeType: currentMimeType,
                                        duration: chunkDuration,
                                        overlap: overlapBuffer.length > 0 ? 200 : 0, // 200ms overlap
                                        timestamp: Date.now(),
                                        retry_attempt: retryAttempt
                                    };
                                    
                                    const logMessage = `[AUDIO] Sending chunk ${chunkSequence}${isLastChunk ? ' (final)' : ''}${retryAttempt > 0 ? ` (retry ${retryAttempt})` : ''}, size: ${audioBlob.size} bytes`;
                                    if (debugMode || retryAttempt > 0) {
                                        console.log(logMessage);
                                    }
                                    
                                    try {
                                        aiWs.send(JSON.stringify(chunkMessage));
                                        console.log(`[AUDIO] ‚úÖ Chunk ${chunkSequence} sent successfully to GPU`);
                                        
                                        // Set up retry timeout (only for non-final chunks)
                                        if (!isLastChunk && retryAttempt === 0) {
                                            const currentChunkSequence = chunkSequence; // Capture current value
                                            const retryTimeout = setTimeout(() => {
                                                // Check if chunk was already acknowledged
                                                if (acknowledgedChunks.has(currentChunkSequence)) {
                                                    console.log(`[AUDIO] Chunk ${currentChunkSequence} already acknowledged, skipping retry`);
                                                    chunkRetryTimeouts.delete(currentChunkSequence);
                                                    return;
                                                }
                                                
                                                const attempts = chunkRetryAttempts.get(currentChunkSequence) || 0;
                                                if (attempts < 2) { // Max 2 retries
                                                    console.log(`[AUDIO] Chunk ${currentChunkSequence} timeout, retrying...`);
                                                    chunkRetryAttempts.set(currentChunkSequence, attempts + 1);
                                                    sendAudioChunk(audioBlob, isLastChunk, attempts + 1);
                                                } else {
                                                    console.error(`[AUDIO] Chunk ${currentChunkSequence} failed after 2 retries`);
                                                    chunkRetryAttempts.delete(currentChunkSequence);
                                                }
                                            }, 5000); // 5 second timeout
                                            
                                            // Store retry timeout reference
                                            chunkRetryTimeouts.set(currentChunkSequence, retryTimeout);
                                        }
                                        
                                        chunkSequence++;
                                    } catch (sendError) {
                                        console.error('[AUDIO] Failed to send chunk:', sendError);
                                        if (retryAttempt < 2) {
                                            setTimeout(() => {
                                                sendAudioChunk(audioBlob, isLastChunk, retryAttempt + 1);
                                            }, 1000 * (retryAttempt + 1)); // Progressive delay
                                        }
                                    }
                                } else {
                                    console.log('[AUDIO] Cannot send chunk - WebSocket not ready, state:', aiWs ? aiWs.readyState : 'null');
                                    
                                    // Retry when WebSocket reconnects
                                    if (retryAttempt < 2) {
                                        setTimeout(() => {
                                            sendAudioChunk(audioBlob, isLastChunk, retryAttempt + 1);
                                        }, 2000);
                                    }
                                }
                            };
                            reader.onerror = (error) => {
                                console.error('[AUDIO] Failed to read chunk:', error);
                                if (retryAttempt < 2) {
                                    setTimeout(() => {
                                        sendAudioChunk(audioBlob, isLastChunk, retryAttempt + 1);
                                    }, 1000);
                                }
                            };
                            reader.readAsDataURL(audioBlob);
                        }
                        
                        // Setup initial event handlers
                        setupMediaRecorderHandlers();
                        
                        console.log('üé§ MediaRecorder initialized successfully');
                        console.log('[AUDIO] Format:', currentMimeType, 'State:', mediaRecorder.state);
                        console.log('[AUDIO] Browser capabilities confirmed:', audioCapabilities.preferred_format, 'supported');
                        
                        // Mark audio detection as successfully set up
                        audioDetectionSetup = true;
                        console.log('[AUDIO] üéØ Audio detection setup complete - won\'t run again');
                        
                    } catch (error) {
                        console.error('[AUDIO] MediaRecorder initialization failed:', error);
                        console.log('[AUDIO] Audio recording will be disabled for this session');
                        mediaRecorder = null;
                        
                        // Update voice indicator to show audio disabled
                        if (voiceStatusDisplay) {
                            voiceStatusDisplay.textContent = 'Audio Recording Disabled';
                            voiceIndicator.className = 'voice-indicator noise';
                            voiceIndicator.style.display = 'block';
                        }
                        
                        // Send error to GPU
                        if (aiWs && aiWs.readyState === WebSocket.OPEN) {
                            aiWs.send(JSON.stringify({
                                type: 'error',
                                error_code: 'AUDIO_002',
                                message: 'MediaRecorder initialization failed: ' + error.message,
                                sessionId: sessionId,
                                timestamp: Date.now()
                            }));
                        }
                        
                        // Continue with audio detection even without recording
                        console.log('[AUDIO] Continuing with visual audio detection only');
                    }
                    
                    microphone.connect(analyser);
                    analyser.fftSize = 256;
                
                let isSpeaking = false;
                let silenceTimer = null;
                let lastSpeechTime = 0;
                const SILENCE_THRESHOLD = 1500; // 1.5 seconds of silence before considering speech ended
                const SPEECH_COOLDOWN = 2000; // 2 seconds cooldown between speeches
                
                // Enhanced voice detection parameters - stable recording sessions
                const VOICE_START_THRESHOLD = 6.0; // Level to START recording
                const VOICE_CONTINUE_THRESHOLD = 4.0; // Level to CONTINUE recording (lower to bridge syllables)
                const MIN_RECORDING_TIME = 300; // Minimum recording duration (300ms as requested)
                const SUSTAINED_SPEECH_TIME = 100; // Quick trigger once threshold met
                const NOISE_GATE_SAMPLES = 5; // Balanced for responsiveness vs stability
                const KEYBOARD_FILTER_TIME = 50; // Keyboard clicks are typically < 50ms
                
                let audioSamples = []; // Rolling buffer for audio level analysis
                let frequencySamples = []; // Rolling buffer for frequency analysis
                let speechStartTime = 0;
                let recordingSessionActive = false; // Track if we're in a recording session
                let recordingSessionStartTime = 0; // When current recording session started
                
                function startChunkedRecording() {
                    if (!mediaRecorder) {
                        console.log('[AUDIO] MediaRecorder not available');
                        return;
                    }
                    
                    console.log('[AUDIO] Current MediaRecorder state:', mediaRecorder.state);
                    
                    // Force stop any existing recording
                    if (mediaRecorder.state === 'recording') {
                        console.log('[AUDIO] MediaRecorder already recording, stopping first');
                        mediaRecorder.stop();
                        isRecording = false;
                        
                        // Wait for it to stop, then restart
                        setTimeout(() => {
                            startChunkedRecording();
                        }, 500);
                        return;
                    }
                    
                    if (mediaRecorder.state === 'paused') {
                        console.log('[AUDIO] MediaRecorder paused, resuming');
                        mediaRecorder.resume();
                        return;
                    }
                    
                    if (mediaRecorder.state !== 'inactive') {
                        console.log('[AUDIO] MediaRecorder not ready, state:', mediaRecorder.state, '- recreating MediaRecorder');
                        
                        // Recreate MediaRecorder using TESTED working formats
                        try {
                            if (workingFormats.length > 0) {
                                // Create fresh audio-only stream
                                const audioOnlyStream = new MediaStream();
                                localStream.getAudioTracks().forEach(track => {
                                    audioOnlyStream.addTrack(track);
                                });
                                
                                const bestFormat = workingFormats[0];
                                const options = bestFormat.requestedFormat ? { mimeType: bestFormat.requestedFormat } : {};
                                mediaRecorder = new MediaRecorder(audioOnlyStream, options);
                                currentMimeType = mediaRecorder.mimeType;
                                console.log('[AUDIO] ‚úÖ Recreated MediaRecorder with tested format:', currentMimeType);
                                
                                // Re-setup event handlers
                                setupMediaRecorderHandlers();
                            } else {
                                console.error('[AUDIO] No working formats available for recreation');
                                return;
                            }
                        } catch (error) {
                            console.error('[AUDIO] Failed to recreate MediaRecorder with tested format:', error);
                            return;
                        }
                    }
                    
                    try {
                        console.log('üé§ Starting chunked audio recording for real-time STT');
                        console.log('[AUDIO] Starting chunked recording - chunks every', chunkDuration + 'ms');
                        console.log('[AUDIO] MediaRecorder state before start:', mediaRecorder.state);
                        console.log('[AUDIO] MediaRecorder mimeType (var):', currentMimeType);
                        console.log('[AUDIO] MediaRecorder mimeType (actual):', mediaRecorder.mimeType);
                        
                        // Reset chunking state
                        chunkSequence = 0;
                        overlapBuffer = [];
                        isRecording = true;
                        recordingStartTime = Date.now();
                        
                        // Start recording with chunk duration (e.g., 3000ms chunks)
                        mediaRecorder.start(chunkDuration);
                        console.log('[AUDIO] ‚úÖ Chunked recording started successfully');
                        
                    } catch (startError) {
                        console.error('[AUDIO] ‚ùå Failed to start chunked recording:', startError);
                        console.log('[AUDIO] Error details:', {
                            name: startError.name,
                            message: startError.message,
                            state: mediaRecorder.state,
                            mimeType: currentMimeType,
                            chunkDuration: chunkDuration,
                            streamActive: localStream && localStream.active,
                            audioTracks: localStream ? localStream.getAudioTracks().length : 0
                        });
                        isRecording = false;
                        
                        // Try to recreate MediaRecorder for next attempt
                        setTimeout(async () => {
                            console.log('[AUDIO] Attempting to recreate MediaRecorder after error...');
                            await initializeMediaRecorder();
                            setupMediaRecorderHandlers();
                        }, 1000);
                    }
                }
                
                // Enhanced voice activity detection with keyboard filtering
                function isVoiceActivity(audioLevel, frequencyData) {
                    // Remove the level check here - handled by calling functions
                    
                    // Analyze frequency distribution for human voice characteristics
                    // Human voice typically has energy in 85Hz - 3000Hz range
                    const voiceFreqStart = Math.floor((85 / (audioContext.sampleRate / 2)) * frequencyData.length);
                    const voiceFreqEnd = Math.floor((3000 / (audioContext.sampleRate / 2)) * frequencyData.length);
                    
                    // Also check high frequencies typical of keyboard clicks (3000Hz+)
                    const highFreqStart = Math.floor((3000 / (audioContext.sampleRate / 2)) * frequencyData.length);
                    
                    let voiceEnergy = 0;
                    let highFreqEnergy = 0;
                    let totalEnergy = 0;
                    
                    for (let i = 0; i < frequencyData.length; i++) {
                        totalEnergy += frequencyData[i];
                        if (i >= voiceFreqStart && i <= voiceFreqEnd) {
                            voiceEnergy += frequencyData[i];
                        }
                        if (i >= highFreqStart) {
                            highFreqEnergy += frequencyData[i];
                        }
                    }
                    
                    const voiceRatio = totalEnergy > 0 ? (voiceEnergy / totalEnergy) : 0;
                    const highFreqRatio = totalEnergy > 0 ? (highFreqEnergy / totalEnergy) : 0;
                    
                    // Keyboard clicks have lots of high frequency energy, voice doesn't
                    const isLikelyKeyboard = highFreqRatio > 0.4 && voiceRatio < 0.3;
                    const passesVoiceRatio = voiceRatio > 0.15;
                    
                    if (isLikelyKeyboard) {
                        // Only log keyboard detection very rarely to avoid spam (once per ~1000 calls)
                        if (Math.random() < 0.001) { // 0.1% chance to log
                            console.log(`‚å®Ô∏è Keyboard detected: high freq ${(highFreqRatio * 100).toFixed(0)}%, voice ${(voiceRatio * 100).toFixed(0)}%`);
                        }
                        return false;
                    }
                    
                    // Debug voice activity check (reduced frequency)
                    if (audioLevel > 6 && Math.random() < 0.05) { // Only 5% of the time to reduce spam
                        console.log(`üé§ Voice activity check:`, {
                            audioLevel: audioLevel.toFixed(1),
                            voiceRatio: voiceRatio.toFixed(3),
                            highFreqRatio: highFreqRatio.toFixed(3),
                            passesVoiceRatio,
                            voiceRatioThreshold: 0.15,
                            isLikelyKeyboard,
                            keyboardThresholds: { highFreq: 0.4, voiceMax: 0.3 },
                            result: passesVoiceRatio && !isLikelyKeyboard
                        });
                    }
                    
                    return passesVoiceRatio;
                }
                
                // Stable recording session management
                function shouldStartRecording(audioLevel, frequencyData) {
                    const levelCheck = audioLevel >= VOICE_START_THRESHOLD;
                    const voiceCheck = isVoiceActivity(audioLevel, frequencyData);
                    const stateCheck = isAudioEnabled && !isAIPaused && !isAISpeaking;
                    
                    // Debug logging to see what's failing (reduced frequency)
                    if (audioLevel > 5 && Math.random() < 0.1) { // Only 10% of the time to reduce spam
                        console.log(`üîç shouldStartRecording check:`, {
                            audioLevel: audioLevel.toFixed(1),
                            threshold: VOICE_START_THRESHOLD,
                            levelCheck,
                            voiceCheck,
                            stateCheck,
                            isAudioEnabled,
                            isAIPaused,
                            isAISpeaking,
                            recordingSessionActive,
                            wouldStart: levelCheck && voiceCheck && stateCheck
                        });
                    }
                    
                    return levelCheck && voiceCheck && stateCheck;
                }
                
                function shouldContinueRecording(audioLevel) {
                    if (!recordingSessionActive) return false;
                    
                    const sessionDuration = Date.now() - recordingSessionStartTime;
                    
                    // Always continue if under minimum time (bridges syllables)
                    if (sessionDuration < MIN_RECORDING_TIME) {
                        return true;
                    }
                    
                    // After minimum time, use continue threshold
                    return audioLevel >= VOICE_CONTINUE_THRESHOLD;
                }
                
                function detectSpeaking() {
                    analyser.getByteFrequencyData(dataArray);
                    const audioLevel = dataArray.reduce((a, b) => a + b) / dataArray.length;
                    
                    // Get frequency data for voice analysis
                    const frequencyData = new Uint8Array(analyser.frequencyBinCount);
                    analyser.getByteFrequencyData(frequencyData);
                    
                    // Add current sample to rolling buffers
                    audioSamples.push(audioLevel);
                    if (audioSamples.length > NOISE_GATE_SAMPLES) {
                        audioSamples.shift();
                    }
                    
                    // Calculate consistency - voice should be relatively stable, keyboard clicks are spiky
                    const avgLevel = audioSamples.reduce((a, b) => a + b) / audioSamples.length;
                    const variance = audioSamples.reduce((acc, val) => acc + Math.pow(val - avgLevel, 2), 0) / audioSamples.length;
                    
                    // Voice is consistent, keyboard clicks are very spiky
                    const isConsistent = variance < 150; // Tightened for better keyboard filtering
                    
                    // Additional check: if all samples are very similar to current level, it's likely sustained voice
                    const sustainedVoice = audioSamples.every(sample => Math.abs(sample - audioLevel) < 3);
                    const isVoiceLike = isConsistent || sustainedVoice;
                    
                    // Check if current audio qualifies as voice activity
                    // High audio override: if audio is moderately loud (>15), bypass some strict checks
                    const highAudioOverride = audioLevel > 15;
                    const passesVoiceCheck = highAudioOverride || isVoiceActivity(audioLevel, frequencyData);
                    const passesConsistencyCheck = highAudioOverride || isVoiceLike;
                    
                    const hasVoiceActivity = passesVoiceCheck && 
                                           passesConsistencyCheck && 
                                           isAudioEnabled && 
                                           !isAIPaused && 
                                           !isAISpeaking;
                    
                    // Update visual voice indicator with diagnostic info
                    audioLevelDisplay.textContent = audioLevel.toFixed(1);
                    
                    // Calculate voice metrics for debugging
                    const voiceFreqStart = Math.floor((85 / (audioContext.sampleRate / 2)) * frequencyData.length);
                    const voiceFreqEnd = Math.floor((3000 / (audioContext.sampleRate / 2)) * frequencyData.length);
                    let voiceEnergy = 0;
                    let totalEnergy = 0;
                    
                    for (let i = 0; i < frequencyData.length; i++) {
                        totalEnergy += frequencyData[i];
                        if (i >= voiceFreqStart && i <= voiceFreqEnd) {
                            voiceEnergy += frequencyData[i];
                        }
                    }
                    const voiceRatio = totalEnergy > 0 ? (voiceEnergy / totalEnergy) : 0;
                    
                    // Calculate variance using the already computed avgLevel and variance from above
                    const currentVariance = variance;
                    
                    if (audioLevel > VOICE_START_THRESHOLD && !isAudioEnabled) {
                        voiceStatusDisplay.textContent = 'Muted';
                        voiceIndicator.className = 'voice-indicator noise';
                        voiceIndicator.style.display = 'block';
                    } else if (isAISpeaking) {
                        voiceStatusDisplay.textContent = 'AI Speaking';
                        voiceIndicator.className = 'voice-indicator';
                        voiceIndicator.style.display = 'block';
                    } else if (recordingSessionActive) {
                        const sessionDuration = Date.now() - recordingSessionStartTime;
                        const status = `Recording: ${sessionDuration}ms (min: ${MIN_RECORDING_TIME}ms)`;
                        voiceStatusDisplay.textContent = status;
                        voiceIndicator.className = 'voice-indicator speaking';
                        voiceIndicator.style.display = 'block';
                    } else if (audioLevel > 5) {
                        // Show why it's not detecting voice and log for debugging
                        let reason = '';
                        if (audioLevel < VOICE_START_THRESHOLD) {
                            reason = `Too quiet (${audioLevel.toFixed(1)} < ${VOICE_START_THRESHOLD})`;
                        } else if (voiceRatio < 0.15) {
                            reason = `Voice ratio: ${voiceRatio.toFixed(2)} < 0.15`;
                        } else if (!isConsistent) {
                            reason = `Variance: ${currentVariance.toFixed(0)} >= 150`;
                        } else if (!isAudioEnabled) {
                            reason = 'Audio disabled';
                        } else if (isAIPaused) {
                            reason = 'AI paused';
                        } else if (isAISpeaking) {
                            reason = 'AI speaking';
                        } else {
                            // More specific debugging for "Other conditions"
                            const wouldStartRecording = shouldStartRecording(audioLevel, frequencyData);
                            if (!wouldStartRecording) {
                                reason = 'shouldStartRecording failed';
                            } else if (currentVariance >= 150) {
                                reason = `High variance: ${currentVariance.toFixed(0)}`;
                            } else {
                                reason = 'Unknown other condition';
                            }
                        }
                        
                        voiceStatusDisplay.textContent = reason;
                        voiceIndicator.className = 'voice-indicator noise';
                        voiceIndicator.style.display = 'block';
                        
                        // Debug logging for troubleshooting
                        if (audioLevel > 6) {
                            console.log(`üîç Audio detected but not voice: ${reason}`, {
                                audioLevel: audioLevel.toFixed(1),
                                voiceRatio: voiceRatio.toFixed(3),
                                variance: currentVariance.toFixed(0),
                                isConsistent,
                                sustainedVoice,
                                isVoiceLike,
                                passesVoiceCheck,
                                passesConsistencyCheck,
                                highAudioOverride
                            });
                        }
                    } else {
                        voiceStatusDisplay.textContent = 'Listening...';
                        voiceIndicator.className = 'voice-indicator';
                        if (audioLevel > 2) {
                            voiceIndicator.style.display = 'block';
                        } else {
                            voiceIndicator.style.display = 'none';
                        }
                    }
                    
                    // NEW STABLE RECORDING SESSION LOGIC
                    
                    // Check if we should start a new recording session
                    if (!recordingSessionActive && shouldStartRecording(audioLevel, frequencyData)) {
                        recordingSessionActive = true;
                        recordingSessionStartTime = Date.now();
                        isSpeaking = true;
                        speechStartTime = Date.now();
                        
                        console.log(`üé§ RECORDING SESSION STARTED - Level: ${audioLevel.toFixed(1)}, Voice Ratio: ${voiceRatio.toFixed(3)}`);
                        console.log(`üé§ Minimum session time: ${MIN_RECORDING_TIME}ms`);
                        
                        updateSpeakingIndicator('local');
                        
                        // Clear any existing silence timer
                        if (silenceTimer) {
                            clearTimeout(silenceTimer);
                            silenceTimer = null;
                        }
                        
                        // Start chunked recording if available
                        if (mediaRecorder && !isRecording) {
                            startChunkedRecording();
                        }
                    }
                    
                    // Check if we should continue the current recording session
                    else if (recordingSessionActive) {
                        const sessionDuration = Date.now() - recordingSessionStartTime;
                        const shouldContinue = shouldContinueRecording(audioLevel);
                        
                        if (shouldContinue) {
                            // Continue recording - clear any silence timer
                            if (silenceTimer) {
                                clearTimeout(silenceTimer);
                                silenceTimer = null;
                            }
                            updateSpeakingIndicator('local');
                        } else {
                            // Start silence timer if not already started
                            if (!silenceTimer) {
                                console.log(`üîá Voice dipped below continue threshold (${audioLevel.toFixed(1)} < ${VOICE_CONTINUE_THRESHOLD.toFixed(1)}) after ${sessionDuration}ms session`);
                                silenceTimer = setTimeout(() => {
                                    // End recording session
                                    recordingSessionActive = false;
                                    isSpeaking = false;
                                    lastSpeechTime = Date.now();
                                    speechStartTime = 0;
                                    
                                    const totalSessionTime = Date.now() - recordingSessionStartTime;
                                    console.log(`üîá RECORDING SESSION ENDED - Total duration: ${totalSessionTime}ms`);
                                    
                                    // Stop chunked recording
                                    if (mediaRecorder && mediaRecorder.state === 'recording' && isRecording) {
                                        try {
                                            console.log('üé§ Stopping chunked recording - session ended');
                                            mediaRecorder.stop();
                                        } catch (stopError) {
                                            console.error('[AUDIO] Failed to stop chunked recording:', stopError);
                                            isRecording = false;
                                        }
                                    }
                                    
                                    updateSpeakingIndicator(null);
                                }, SILENCE_THRESHOLD);
                            }
                        }
                    }
                    
                    // Reset speech start time if no activity and not in session
                    if (!recordingSessionActive && !shouldStartRecording(audioLevel, frequencyData)) {
                        speechStartTime = 0;
                    }
                    
                    requestAnimationFrame(detectSpeaking);
                }
                
                detectSpeaking();
                console.log('üéß Audio detection started successfully');
                
                } catch (error) {
                    console.error('‚ùå Audio detection setup failed:', error);
                }
            } else {
                console.log('‚ùå No local stream available for audio detection');
            }
        }

        // Cleanup on page unload
        window.addEventListener('beforeunload', () => {
            if (aiWs) {
                aiWs.send(JSON.stringify({
                    type: 'end_session',
                    sessionId: sessionId
                }));
                aiWs.close();
            }
            
            if (localStream) {
                localStream.getTracks().forEach(track => track.stop());
            }
            
            if (peerConnection) {
                peerConnection.close();
            }
        });

        // Start the application
        initializeMedia();
        
        // Set up audio detection after media is ready
        setTimeout(async () => {
            console.log('Setting up audio detection after delay...');
            await setupAudioDetection();
        }, 2000);
        
        // Demo mode completely removed - using real speech detection only
        console.log('AI room initialized - ready for real speech interaction');
        console.log('');
        console.log('üîç DEBUGGING INFO:');
        console.log('Browser location:', window.location.href);
        console.log('Protocol:', window.location.protocol);
        console.log('Host:', window.location.host);
        console.log('SessionId:', sessionId);
        console.log('Look for these key log messages:');
        console.log('  [AI-WS] Connecting to GPU server via CPU proxy: [URL]');
        console.log('  [AI-WS] WebSocket created, initial readyState: 0');
        console.log('  [AI-WS] GPU connection status: SUCCESS');
        console.log('  [AI-WS] Sending to GPU: {"type":"client_speaking_detected"...}');
        console.log('  [AI-WS] Received from GPU: {"type":"client_speaking"...}');
        console.log('  [AI-WS] GPU disconnected, reason: [reason]');
        console.log('');
    </script>
</body>
</html>